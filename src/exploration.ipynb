{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"d3dc1998-b012-4a1b-bde8-9e4be19db352\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"d3dc1998-b012-4a1b-bde8-9e4be19db352\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"d3dc1998-b012-4a1b-bde8-9e4be19db352\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Using Media Cloud python client v4.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up your API key and import needed things\n",
    "import os, mediacloud.api\n",
    "from importlib.metadata import version\n",
    "from dotenv import load_dotenv\n",
    "import datetime as dt\n",
    "from IPython.display import JSON\n",
    "import bokeh.io\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()\n",
    "MC_API_KEY = 'fd180b0f9ce16f29121870731d39725c24f094e5'\n",
    "search_api = mediacloud.api.SearchApi(MC_API_KEY)\n",
    "f'Using Media Cloud python client v{version(\"mediacloud\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'id': 'bbe590fcc23edb5d310ef99dd1dbddeb1223ff634588d2043ba18e80789700cf',\n",
       "   'indexed_date': datetime.datetime(2025, 3, 3, 11, 28, 11, 454314, tzinfo=datetime.timezone.utc),\n",
       "   'language': 'en',\n",
       "   'media_name': 'bostonglobe.com',\n",
       "   'media_url': 'bostonglobe.com',\n",
       "   'publish_date': datetime.date(2025, 3, 3),\n",
       "   'title': 'As warming climate hammers coffee crops, this rare bean may someday be your brew',\n",
       "   'url': 'https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/'}],\n",
       " None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many stories include the phrase \"climate change\" in the Washington Post (media id #2)\n",
    "my_query = 'url:\"https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6,10)\n",
    "search_api.story_list(my_query,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_globe_article = search_api.story('bbe590fcc23edb5d310ef99dd1dbddeb1223ff634588d2043ba18e80789700cf')['text']\n",
    "with open(\"boston_globe_article.txt\", \"w\") as file:\n",
    "    file.write(boston_globe_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.08\n",
      "Subjectivity: 0.40\n",
      "{'neg': 0.082, 'neu': 0.83, 'pos': 0.088, 'compound': 0.2392}\n",
      "Overall sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "#%pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "with open('boston_globe_article.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores['compound']\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to yale_360_article.txt\n",
      "Polarity: 0.12\n",
      "Subjectivity: 0.37\n",
      "{'neg': 0.053, 'neu': 0.885, 'pos': 0.062, 'compound': 0.9771}\n",
      "Overall sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://e360.yale.edu/features/eric-nost-interview' \n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Example: Extracting all h2 tags\n",
    "    headings = soup.find_all('p')\n",
    "    extracted_data = [h.text.strip() for h in headings]\n",
    "\n",
    "    with open('yale_360_article.txt', 'w', encoding='utf-8') as f:\n",
    "        for heading in extracted_data:\n",
    "            f.write(heading + '\\n')\n",
    "\n",
    "    print(\"Data scraped and saved to yale_360_article.txt\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching URL: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "with open('yale_360_article.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores['compound']\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to bbc_article.txt\n",
      "Polarity: 0.06\n",
      "Subjectivity: 0.47\n",
      "{'neg': 0.07, 'neu': 0.854, 'pos': 0.076, 'compound': -0.9091}\n",
      "Overall sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bbc.com/news/articles/ce82p6yq061o' \n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Example: Extracting all h2 tags\n",
    "    headings = soup.find_all('p')\n",
    "    extracted_data = [h.text.strip() for h in headings]\n",
    "\n",
    "    with open('bbc_article.txt', 'w', encoding='utf-8') as f:\n",
    "        for heading in extracted_data:\n",
    "            f.write(heading + '\\n')\n",
    "\n",
    "    print(\"Data scraped and saved to bbc_article.txt\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching URL: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "with open('bbc_article.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores['compound']\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to bbc_article.txt\n",
      "Polarity: 0.05\n",
      "Subjectivity: 0.39\n",
      "{'neg': 0.078, 'neu': 0.801, 'pos': 0.121, 'compound': 0.9952}\n",
      "Overall sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#url = 'https://www.bbc.com/news/articles/ce82p6yq061o' \n",
    "#url = 'https://www.bbc.com/news/articles/c1dekp93l6po'\n",
    "url = 'https://www.bbc.com/news/articles/cvgdyl817p1o'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Example: Extracting all h2 tags\n",
    "    headings = soup.find_all('p')\n",
    "    extracted_data = [h.text.strip() for h in headings]\n",
    "\n",
    "    with open('bbc_article.txt', 'w', encoding='utf-8') as f:\n",
    "        for heading in extracted_data:\n",
    "            f.write(heading + '\\n')\n",
    "\n",
    "    print(\"Data scraped and saved to bbc_article.txt\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching URL: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "with open('bbc_article.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores['compound']\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'a4b9491b4f15c523ddd4121978664fd4c7a93d3909f99ba1ecfc9aa45b497675', 'indexed_date': datetime.datetime(2025, 6, 10, 21, 40, 17, 122666, tzinfo=datetime.timezone.utc), 'language': 'en', 'media_name': 'bbc.com', 'media_url': 'bbc.com', 'publish_date': datetime.date(2025, 6, 9), 'title': 'Homeowners warned over green energy scammers', 'url': 'https://www.bbc.com/news/articles/cvgdyl817p1o'}]\n",
      "YAY\n"
     ]
    }
   ],
   "source": [
    "my_query = 'url:\"https://www.bbc.com/news/articles/cvgdyl817p1o\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6,10)\n",
    "print(search_api.story_list(my_query,start_date,end_date)[0])\n",
    "\n",
    "my_query = 'url:\"https://e360.yale.edu/features/eric-nost-interview\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6,10)\n",
    "x = search_api.story_list(my_query,start_date,end_date)\n",
    "if len(x[0]) == 0:\n",
    "    print(\"YAY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "\n",
    "def extract_full_text(url, output_file_name):\n",
    "    my_query = f'url:\"{url}\"'\n",
    "    start_date = dt.date(2025, 1, 1)\n",
    "    end_date = dt.date(2025, 6, 10)\n",
    "    \n",
    "    try:\n",
    "        result = search_api.story_list(my_query, start_date, end_date)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Search API failed: {e}\")\n",
    "        result = []\n",
    "\n",
    "    article = \"\"\n",
    "    if result:\n",
    "        try:\n",
    "            article = search_api.story(result[0]['id'])['text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching from Media Cloud: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Try to find main content first\n",
    "            main_content = soup.find('article') or soup.find('div', class_='main-content') or soup\n",
    "            p_tags = main_content.find_all('p')\n",
    "            article = \"\\n\".join(p.get_text(strip=True) for p in p_tags if p.get_text(strip=True))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "    \n",
    "    out_file = f\"{output_file_name}.txt\"\n",
    "    with open(out_file, \"w\", encoding='utf-8') as file:\n",
    "        file.write(article)\n",
    "\n",
    "import difflib\n",
    "\n",
    "def compare_files(file1_path, file2_path):\n",
    "    \"\"\"Compares two text files and prints the differences.\"\"\"\n",
    "    with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
    "        file1_lines = file1.readlines()\n",
    "        file2_lines = file2.readlines()\n",
    "\n",
    "    diff = difflib.unified_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path)\n",
    "\n",
    "    for line in diff:\n",
    "        print(line, end=\"\")\n",
    "\n",
    "#def sentiment_analysis(file):\n",
    "    #VADER - pos/neg/neutral\n",
    "    #textblob - Polarity/Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_full_text(\"https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/\",\"boston_globe_article_f\")\n",
    "compare_files('boston_globe_article_f.txt', 'boston_globe_article.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- yale_360_article_f.txt\n",
      "+++ yale_360_article.txt\n",
      "@@ -1,52 +1,61 @@\n",
      "-Monitors at NOAA’s Weather and Climate Prediction headquarters in College Park, Maryland.Michael A. McCoy / Bloomberg via Getty Images\n",
      "+\n",
      "+/\n",
      "+← →\n",
      "+Monitors at NOAA’s Weather and Climate Prediction headquarters in College Park, Maryland. \n",
      "+Michael A. McCoy / Bloomberg via Getty Images\n",
      " Some 2,000 records went missing from government data sets after the Trump administration took office in January. Canadian geographer Eric Nost talks about the work he and colleagues are doing to archive data related to climate and the environment while it is still accessible.\n",
      "-ByNicola Jones•March 11, 2025\n",
      "+By  Nicola Jones\n",
      "+ •\n",
      "+March 11, 2025\n",
      " Since Donald Trump returned to the White House, thousands of government data sets have been altered or removed, including key tools that researchers and policymakers use to track which communities are most at risk from climate change and toxic hazards.\n",
      " Eric Nost is a geographer and policy scholar at the University of Guelph in Canada who has been working with the U.S.-based Environmental Data and Governance Initiative to help track and back up these resources before they are lost. He says while every administration change comes with website alterations and shifts to how data is presented or organized, this time things are very different.\n",
      "-“When you start taking down this information, changing how issues are described and doing so in misleading ways,” he says, “really, what it is, is censorship and propaganda.” He spoke toYale Environment 360about his efforts.\n",
      "-Yale Environment 360:What did you and your colleagues at the Environmental Data and Governance Initiative start doing when it became clear in 2024 that Trump might be returning to the White House?\n",
      "-Eric Nost.University of Guelph\n",
      "-Eric Nost:Given the possibility of Trump being elected again, there was planning in the works as the Biden administration came to a close. The Wayback Machine, which takes snapshots of pages over time, is great at capturing static web pages, not so great at archiving the data sets you need to click on and download. So, we began reaching out and working with partners, eventually coming to call ourselves thePublic Environmental Data Partners. We developed a list of several hundred data sets across U.S. federal agencies that we used frequently and also did a public solicitation. That list turned out to be several hundred data sets, which is a lot. From this we whittled down to a list of 60 data sets we felt were really at risk, which we divvied up and archived in a variety of ways, downloading it and making it available one way or another.\n",
      "-e360:What has happened since Trump returned to office in January?\n",
      "-Nost:Data.gov is a central repository of government data sets that indexes things and makes them easier to find: As of the end of January, around 2,000 records hadgone missingon Data.gov, out of a grand total of about 308,000. It doesn’t necessarily mean that the data are gone from the record forever. But it’s certainly not good, because it’s making it all less available.\n",
      "-Subscribe to the E360 Newsletter for weekly updates delivered to your inbox.Sign Up.\n",
      "-Trump’s first actions from day one were really targeted at DEI [diversity, equity, and inclusion], in addition to basic rescinding of involvement in climate action, such as the Paris Agreement. That has really directed what agencies have done. The 25-year-old climate change center at the Department of Transportation, for example, was very quick to go. The EPAremoved“climate change” from its website navigation, making it harder to find those pages.\n",
      "+“When you start taking down this information, changing how issues are described and doing so in misleading ways,” he says, “really, what it is, is censorship and propaganda.” He spoke to Yale Environment 360 about his efforts.\n",
      "+Yale Environment 360: What did you and your colleagues at the Environmental Data and Governance Initiative start doing when it became clear in 2024 that Trump might be returning to the White House?\n",
      "+Eric Nost. \n",
      "+University of Guelph\n",
      "+Eric Nost: Given the possibility of Trump being elected again, there was planning in the works as the Biden administration came to a close. The Wayback Machine, which takes snapshots of pages over time, is great at capturing static web pages, not so great at archiving the data sets you need to click on and download. So, we began reaching out and working with partners, eventually coming to call ourselves the Public Environmental Data Partners. We developed a list of several hundred data sets across U.S. federal agencies that we used frequently and also did a public solicitation. That list turned out to be several hundred data sets, which is a lot. From this we whittled down to a list of 60 data sets we felt were really at risk, which we divvied up and archived in a variety of ways, downloading it and making it available one way or another.\n",
      "+e360: What has happened since Trump returned to office in January?\n",
      "+Nost: Data.gov is a central repository of government data sets that indexes things and makes them easier to find: As of the end of January, around 2,000 records had gone missing on Data.gov, out of a grand total of about 308,000. It doesn’t necessarily mean that the data are gone from the record forever. But it’s certainly not good, because it’s making it all less available.\n",
      "+Subscribe to the E360 Newsletter for weekly updates delivered to your inbox. Sign Up.\n",
      "+Trump’s first actions from day one were really targeted at DEI [diversity, equity, and inclusion], in addition to basic rescinding of involvement in climate action, such as the Paris Agreement. That has really directed what agencies have done. The 25-year-old climate change center at the Department of Transportation, for example, was very quick to go. The EPA removed “climate change” from its website navigation, making it harder to find those pages.\n",
      " The biggest set of changes we have seen is the removal of language on environmental justice and DEI — the language and data and tools. What we haven’t seen so far is pure biophysical climate data removal. It’s not like entire NASA climate data sets have gone missing. We tend to be very wary of saying the data are gone or completely missing. Every agency has its internal data systems, and surely it’s on someone’s laptop somewhere. Whether it’s actually publicly available is a whole separate question. There are data sets — like, for instance, a set of records related to grants that the EPA makes or has made for environmental justice purposes — that I can’t find anymore, as hard as I’ve tried. I don’t think it exists anywhere on the internet.\n",
      "-e360:What important things have been removed?\n",
      "-Nost:It’s not just data that matters, but also code for websites that people interact with. There are a lot of important tools out there. Even if data is there somewhere, it’s not much use if it’s all zipped up in a file.\n",
      "-The Climate and Economic Justice Screening Tool was developed under Biden’sJustice40 initiative, which was an early executive order under his administration that sought to allocate 40 percent of climate investments to so-called disadvantaged communities. This tool helped to identify those communities. Trump rescinded Biden’s executive order, and the toolcame downwithin 72 hours of Trump’s inauguration.\n",
      "-But the thing had been developed as an open source tool from day one, and so the code was already available. So we had already made a copy. We were able to fairly readilyrebuild the tooland host it on our ownwebsite.\n",
      "-Similarly,EJScreenwas the EPA’s tool for similarly trying to understand communities that are on the front lines of toxic pollutants. There are a lot of problems with EJScreen, certainly there’s a lot of limitations. But that tool was well used by community organizations, by state and local governments, for grant-writing or communications or advocacy. It had been around since the Obama administration; it had survived the first Trump administration. But that was taken down on February 5. Again, because the code had been made public, we had been able to either make copies of that data or, in some cases, reverse engineered what the data should have been. We were able tomake a copyand make it pretty much fully functional again.\n",
      "-CLICK TO ENLARGE. Left: FEMA's climate resilience website in December. Right: The same site after Trump took office in January, with explicit references to \"climate change\" removed.WaybackMachine\n",
      "-e360:When did you first get involved tracking government data?\n",
      "-Nost:Eight years ago, at the start of Trump’s first administration, I joined the Environmental Data and Governance Initiative [EDGI, pronounced “edgy”]. That group was formed immediately after November 2016 and Trump’s first election, taking histhreatto dismantle the EPA seriously. EDGI is the main organization tracking changes to government websites.\n",
      "+e360: What important things have been removed?\n",
      "+Nost: It’s not just data that matters, but also code for websites that people interact with. There are a lot of important tools out there. Even if data is there somewhere, it’s not much use if it’s all zipped up in a file.\n",
      "+The Climate and Economic Justice Screening Tool was developed under Biden’s Justice40 initiative, which was an early executive order under his administration that sought to allocate 40 percent of climate investments to so-called disadvantaged communities. This tool helped to identify those communities. Trump rescinded Biden’s executive order, and the tool came down within 72 hours of Trump’s inauguration.\n",
      "+But the thing had been developed as an open source tool from day one, and so the code was already available. So we had already made a copy. We were able to fairly readily rebuild the tool and host it on our own website.\n",
      "+Similarly, EJScreen was the EPA’s tool for similarly trying to understand communities that are on the front lines of toxic pollutants. There are a lot of problems with EJScreen, certainly there’s a lot of limitations. But that tool was well used by community organizations, by state and local governments, for grant-writing or communications or advocacy. It had been around since the Obama administration; it had survived the first Trump administration. But that was taken down on February 5. Again, because the code had been made public, we had been able to either make copies of that data or, in some cases, reverse engineered what the data should have been. We were able to make a copy and make it pretty much fully functional again.\n",
      "+CLICK TO ENLARGE. Left: FEMA's climate resilience website in December. Right: The same site after Trump took office in January, with explicit references to \"climate change\" removed.\n",
      "+Wayback Machine\n",
      "+e360: When did you first get involved tracking government data?\n",
      "+Nost: Eight years ago, at the start of Trump’s first administration, I joined the Environmental Data and Governance Initiative [EDGI, pronounced “edgy”]. That group was formed immediately after November 2016 and Trump’s first election, taking his threat to dismantle the EPA seriously. EDGI is the main organization tracking changes to government websites.\n",
      " e360: What does the work with EDGI involve?\n",
      "-Nost:We work with the Internet Archive and the Wayback Machine. Basically, every week — and in the early days after Trump’s first win, twice a week — we would generate reports that were spreadsheets of links to government pages. We would then pull these up in our custom software and volunteers would look to see: Has anything changed? And if so, is that page or is that change significant?\n",
      "-e360:Who funds EDGI?\n",
      "-Nost:A variety offunders, mostly foundations. EDGI has been funded by the National Science Foundation; that funding has been paid out. Perhaps there could have been an issue if the timing had been different.\n",
      "-e360:Does it help being a Canadian doing this work and not someone in the U.S.?\n",
      "-Nost:With all the changes to research funding at NSF [National Science Foundation] and beyond, it does seem quite tough for some of my colleagues in the States to be working productively on some of these things. There may be more leeway for someone not in the U.S. to be doing this kind of work.\n",
      "-Bill McKibben on climate activism in the age of Trump 2.0.Read more.\n",
      "-e360:What happened during Trump’s first administration?\n",
      "-Nost:The overall story from Trump 1.0 was we didn’t see a whole lot of data go missing, per se. But what we did see was the removal and alteration of many different web resources. So, web pages were taken offline. Text on web pages was modified to change how climate issues were presented, often weakening climate language towards more palatable but perhaps vague statements. We saw a lot of that. We published areportin 2019.\n",
      "-e360:Have the courts gotten involved with the issue of missing data and resources?\n",
      "-Nost:This is not something we really saw in the first Trump administration, but we are now seeing legal activism. The USDA [U.S. Department of Agriculture], for instance, took down a number of climate-related resources, and now a group of farmers is suing to have those resources restored. This follows a similar lawsuit filed by doctors against the CDC [Centers for Disease Control and Prevention] when it removed a bunch of public health data sets and web pages. They were able to successfullyarguethat the CDC has to bring it back in the interest of public health, and theyhave done so.\n",
      "-President Trump signs an executive order flanked by members of his cabinet on February 14.Andrew Harnik / Getty Images\n",
      "-e360:Are any records protected?\n",
      "-Nost:One of the reasons why we haven’t seen a lot of biophysical climate data go missing yet is because a lot of it is congressionally mandated. For instance, reports from industry on how many greenhouse gasses they release each year — that’s congressionally mandated. Same thing with a lot of NOAA data sets. Which isn’t to say that the Trump administration won’t flout those at some point, as it has with some other congressional requirements, like USAID [U.S. Agency for International Development] funding. But that’s why we don’t see a lot of that data going missing — yet.\n",
      "-e360:Of course it’s not just data going missing, but peoples’ jobs. Thousands of government workers, including research scientists, have been fired. Any comment about that?\n",
      "-Nost:It’s really disheartening. What we’re seeing is a real gutting of public infrastructure and public capacity. That’s both in terms of people and data sets. Those are investments, in people and data, that the public has made over decades. What are we gutting it for? A tax cut for the wealthy?\n",
      "+Nost: We work with the Internet Archive and the Wayback Machine. Basically, every week — and in the early days after Trump’s first win, twice a week — we would generate reports that were spreadsheets of links to government pages. We would then pull these up in our custom software and volunteers would look to see: Has anything changed? And if so, is that page or is that change significant?\n",
      "+e360: Who funds EDGI?\n",
      "+Nost: A variety of funders, mostly foundations. EDGI has been funded by the National Science Foundation; that funding has been paid out. Perhaps there could have been an issue if the timing had been different.\n",
      "+e360: Does it help being a Canadian doing this work and not someone in the U.S.?\n",
      "+Nost: With all the changes to research funding at NSF [National Science Foundation] and beyond, it does seem quite tough for some of my colleagues in the States to be working productively on some of these things. There may be more leeway for someone not in the U.S. to be doing this kind of work.\n",
      "+Bill McKibben on climate activism in the age of Trump 2.0. Read more.\n",
      "+e360: What happened during Trump’s first administration?\n",
      "+Nost: The overall story from Trump 1.0 was we didn’t see a whole lot of data go missing, per se. But what we did see was the removal and alteration of many different web resources. So, web pages were taken offline. Text on web pages was modified to change how climate issues were presented, often weakening climate language towards more palatable but perhaps vague statements. We saw a lot of that. We published a report in 2019.\n",
      "+e360: Have the courts gotten involved with the issue of missing data and resources?\n",
      "+Nost: This is not something we really saw in the first Trump administration, but we are now seeing legal activism. The USDA [U.S. Department of Agriculture], for instance, took down a number of climate-related resources, and now a group of farmers is suing to have those resources restored. This follows a similar lawsuit filed by doctors against the CDC [Centers for Disease Control and Prevention] when it removed a bunch of public health data sets and web pages. They were able to successfully argue that the CDC has to bring it back in the interest of public health, and they have done so.\n",
      "+President Trump signs an executive order flanked by members of his cabinet on February 14. \n",
      "+Andrew Harnik / Getty Images\n",
      "+e360: Are any records protected?\n",
      "+Nost: One of the reasons why we haven’t seen a lot of biophysical climate data go missing yet is because a lot of it is congressionally mandated. For instance, reports from industry on how many greenhouse gasses they release each year — that’s congressionally mandated. Same thing with a lot of NOAA data sets. Which isn’t to say that the Trump administration won’t flout those at some point, as it has with some other congressional requirements, like USAID [U.S. Agency for International Development] funding. But that’s why we don’t see a lot of that data going missing — yet.\n",
      "+e360: Of course it’s not just data going missing, but peoples’ jobs. Thousands of government workers, including research scientists, have been fired. Any comment about that?\n",
      "+Nost: It’s really disheartening. What we’re seeing is a real gutting of public infrastructure and public capacity. That’s both in terms of people and data sets. Those are investments, in people and data, that the public has made over decades. What are we gutting it for? A tax cut for the wealthy?\n",
      " Those people are the ones who make the data, collect it, and steward it. NOAA might be mandated to collect certain data, but it’s not mandated to have a certain-sized workforce. If there’s not the people there to produce the data, we get gaps in the records.\n",
      "-e360:What else is different for this administration?\n",
      "-Nost:One of the things that we see this time around that we didn’t see the first time around is the removal of access to information from folks outside the U.S. Like with FEMA, the county-by-countytrackingof risk to natural disasters is no longer available to folks outside of the U.S.\n",
      "-e360:Why do these changes matter?\n",
      "-Nost:This Trump administration has undone 30 years of environmental justice work by rescinding Clinton’s 1994 executive order thatrequiredagencies to make achieving EJ a part of their mission. By getting the EPA to evaluate, for example, whether it is permitting polluters disproportionately in Black communities. It didn’t necessarily have a whole lot of teeth, and a lot of complaints were written about it over the years, but at least it was there. That was gone on day one.\n",
      "+e360: What else is different for this administration?\n",
      "+Nost: One of the things that we see this time around that we didn’t see the first time around is the removal of access to information from folks outside the U.S. Like with FEMA, the county-by-county tracking of risk to natural disasters is no longer available to folks outside of the U.S.\n",
      "+e360: Why do these changes matter?\n",
      "+Nost: This Trump administration has undone 30 years of environmental justice work by rescinding Clinton’s 1994 executive order that required agencies to make achieving EJ a part of their mission. By getting the EPA to evaluate, for example, whether it is permitting polluters disproportionately in Black communities. It didn’t necessarily have a whole lot of teeth, and a lot of complaints were written about it over the years, but at least it was there. That was gone on day one.\n",
      " The U.S. government is the world’s biggest publisher. People from around the world turn to it as a source of information. So when you start taking down this information, changing how issues are described, and doing so in misleading ways, really, what it is, is censorship and propaganda.\n",
      "-Can support for clean energy withstand changing political winds?Read more.\n",
      "-e360:What can or should people be doing?\n",
      "-Nost:Advocacy for policy change. Joining organizations like EDGI — we would always like more volunteers. Even if you have one website you routinely use, make a copy on theWayback Machine. It’s easy — just put the link it. It helps build this public backup and public record.\n",
      "-Nicola Jonesis aYale Environment 360contributing editor and a freelance journalist based in Pemberton, British Columbia. With a background in chemistry and oceanography, she writes about the physical sciences, most often for the journalNature. She has also contributed toScientific American,Globe and Mail, andNew Scientistand serves as the science journalist in residence at the University of British Columbia.Moreabout Nicola Jones→\n",
      "+Can support for clean energy withstand changing political winds? Read more.\n",
      "+e360: What can or should people be doing?\n",
      "+Nost: Advocacy for policy change. Joining organizations like EDGI — we would always like more volunteers. Even if you have one website you routinely use, make a copy on the Wayback Machine. It’s easy — just put the link it. It helps build this public backup and public record.\n",
      "+Nicola Jones is a Yale Environment 360 contributing editor and a freelance journalist based in Pemberton, British Columbia. With a background in chemistry and oceanography, she writes about the physical sciences, most often for the journal Nature. She has also contributed to Scientific American, Globe and Mail, and New Scientist and serves as the science journalist in residence at the University of British Columbia. More about Nicola Jones →\n",
      " Never miss an article. Subscribe to the E360 Newsletter →\n",
      "-ByChristian Schwägerl\n",
      "-ByStefan Lovgren\n",
      "-ByMichael Grunwald+By  Christian Schwägerl\n",
      "+By  Stefan Lovgren\n",
      "+By  Michael Grunwald\n"
     ]
    }
   ],
   "source": [
    "extract_full_text(\"https://e360.yale.edu/features/eric-nost-interview\",\"yale_360_article_f\")\n",
    "compare_files('yale_360_article_f.txt','yale_360_article.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- bbc_article_f.txt\n",
      "+++ bbc_article.txt\n",
      "@@ -1,4 +1,3 @@\n",
      "-Homeowners warned over green energy scammers\n",
      " Homeowners need stronger protections against rogue traders and scammers capitalising on green energy schemes, according to Scotland's consumer body.\n",
      " Consumer Scotland warned that more people were likely to be targeted with scams such as unsuitable insulation as the country transitions to low or zero-emissions heating over the next two decades.\n",
      " It has called on both Scottish and UK governments to ensure stronger regulation of the industry including accreditation for traders and a streamlined complaints process.\n",
      "@@ -6,7 +5,7 @@\n",
      " Craig McClue, head of investigations for Consumer Scotland, said: \"When we looked at this sector, in particular, we realised that Scotland has climate change targets to be net zero by 2045 and that is going to drive demand for low carbon heating technologies and insulation products.\n",
      " \"We found that, historically, there have been a lot of scams and unfair trading, and we heard evidence from our enforcement partners about just how prevalent these problems are.\n",
      " \"When you realise more than two million households are soon going to enter this sector, the risk is very real that consumers can face scams and unfair trading.\"\n",
      "-'I've been a sucker... I should've known better'\n",
      "+\n",
      " Among those who have lost out financially by trying to upgrade their homes is 87-year-old David Adams, a widower who lives alone in Glasgow.\n",
      " \"I committed the cardinal sin,\" he reflects. \"Someone chapped my door and I let them in.\"\n",
      " In late 2023, a representative of a company called Smarter Insulation Ltd appeared at his front door asking to talk to him about spray foam insulation.\n",
      "@@ -33,7 +32,6 @@\n",
      " \"I deeply regret that customers have had to endure such a poor experience while purchasing insulation.\"\n",
      " The BBC has been unable to reach RAS for comment.\n",
      " Trading Standards Scotland confirmed that RAS, both Smarter Insulation and FastFoam, and the company that removed Mr Adams' insulation are all currently being investigated.\n",
      "-How do scammers operate?\n",
      " Craig McClue said Consumer Scotland's investigations had found strong evidence of rogue traders targeting vulnerable consumers.\n",
      " \"They suggest a loft surveys and suggest they make immediate decisions to get insulation, which proves unsuitable for the home, creates damp, and leads to mould,\" he said.\n",
      " \"We then hear these same traders return under a new guise to take the spray foam insulation out. It is almost like a life-cycle scam.\"\n",
      "@@ -46,4 +44,11 @@\n",
      " A UK government spokesperson said: \"Allegations of fraud are taken very seriously and for UK wide schemes there is a dedicated Ofgem team working on counter fraud and whistleblowing.\"\n",
      " A Scottish government spokesperson said it welcomed the report \"which highlights the need for a change to clean heating as part of Scotland's commitment to achieving net zero\".\n",
      " They added: \"It is vital people feel assured that any work carried out to decarbonise their heating is done to a high standard and represents good value for money.\n",
      "-\"That's why we have been working with the UK government and Ofgem on the introduction of new consumer standards for heat network consumers – such as fair pricing and reliable supply – which will be introduced in January 2026.\"+\"That's why we have been working with the UK government and Ofgem on the introduction of new consumer standards for heat network consumers – such as fair pricing and reliable supply – which will be introduced in January 2026.\"\n",
      "+More cash for defence, computing and the development of carbon-capture technology are in the Spending Review.\n",
      "+Factories at Falkirk and Larbert are under threat under relocation plans announced by Alexander Dennis\n",
      "+The former net zero and energy secretary, tipped as a future SNP leader, rejoins the cabinet after maternity leave.\n",
      "+The fall-out from the SNP's by-election defeat and a call for more football banning orders make the front pages.\n",
      "+A breastfeeding support service which has helped mums of newborn babies for a decade has closed down.\n",
      "+Copyright 2025 BBC. All rights reserved.  The BBC is not responsible for the content of external sites. Read about our approach to external linking.\n",
      "+\n"
     ]
    }
   ],
   "source": [
    "extract_full_text(\"https://www.bbc.com/news/articles/cvgdyl817p1o\",\"bbc_article_f\")\n",
    "compare_files('bbc_article_f.txt','bbc_article.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttblogs.com\n",
      "blogdomago.com\n",
      "amarujala.com\n",
      "yahoo.com\n",
      "indiatimes.com\n",
      "latestnigeriannews.com\n",
      "benzinga.com\n",
      "dailymail.co.uk\n",
      "yardbarker.com\n",
      "365project.org\n",
      "hindustantimes.com\n",
      "mirror.co.uk\n",
      "globenewswire.com\n",
      "thesun.co.uk\n",
      "kdhnews.com\n",
      "sktoday.com\n",
      "freerepublic.com\n",
      "noticiasya.com\n",
      "india.com\n",
      "express.co.uk\n",
      "finanznachrichten.de\n",
      "independent.co.uk\n",
      "prnewswire.com\n",
      "screenrant.com\n",
      "forbes.com\n",
      "wvnews.com\n",
      "wtop.com\n",
      "metroseoul.co.kr\n",
      "cbsnews.com\n",
      "wellandtribune.ca\n",
      "niagarafallsreview.ca\n",
      "thespec.com\n",
      "ctvnews.ca\n",
      "therecord.com\n",
      "thehindu.com\n",
      "the-messenger.com\n",
      "indianexpress.com\n",
      "nst.com.my\n",
      "einpresswire.com\n",
      "upstract.com\n",
      "theguardian.com\n",
      "kesq.com\n",
      "gazette.com\n",
      "keyt.com\n",
      "news18.com\n",
      "ktvz.com\n",
      "livemint.com\n",
      "dailyrecord.co.uk\n",
      "kion546.com\n",
      "sportingnews.com\n",
      "abc17news.com\n",
      "nytimes.com\n",
      "independent.ie\n",
      "thestar.com\n",
      "citynews.ca\n",
      "seznam.cz\n",
      "newsday.com\n",
      "mediaindonesia.com\n",
      "krdo.com\n",
      "nypost.com\n",
      "localnews8.com\n",
      "biztoc.com\n",
      "urdupoint.com\n",
      "winnipegfreepress.com\n",
      "the-sun.com\n",
      "zawya.com\n",
      "newsweek.com\n",
      "cbssports.com\n",
      "regionalmedianews.com\n",
      "manchestereveningnews.co.uk\n",
      "nature.com\n",
      "standard.co.uk\n",
      "marketbeat.com\n",
      "newindianexpress.com\n",
      "freepressjournal.in\n",
      "ndtv.com\n",
      "metro.co.uk\n",
      "perthnow.com.au\n",
      "tribuneindia.com\n",
      "sangbadpratidin.in\n",
      "birminghammail.co.uk\n",
      "sportskeeda.com\n",
      "menafn.com\n",
      "usatoday.com\n",
      "lakelandtoday.ca\n",
      "burnabynow.com\n",
      "townandcountrytoday.com\n",
      "richmond-news.com\n",
      "westernwheel.ca\n",
      "stalbertgazette.com\n",
      "piquenewsmagazine.com\n",
      "nsnews.com\n",
      "tricitynews.com\n",
      "rmoutlook.com\n",
      "bowenislandundercurrent.com\n",
      "tayyar.org\n",
      "dailystar.co.uk\n",
      "prpeak.com\n",
      "thewest.com.au\n",
      "foxnews.com\n"
     ]
    }
   ],
   "source": [
    "my_query = 'All'\n",
    "start_date = dt.date(2024, 1, 1)\n",
    "end_date = dt.date(2025, 6,10)\n",
    "source_list = search_api.sources(my_query,start_date,end_date)\n",
    "import re\n",
    "for i in source_list:\n",
    "    print(i['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
