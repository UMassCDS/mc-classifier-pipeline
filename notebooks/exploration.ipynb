{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"ac618d29-3145-4d33-b206-7772d37300f9\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"ac618d29-3145-4d33-b206-7772d37300f9\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"ac618d29-3145-4d33-b206-7772d37300f9\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Using Media Cloud python client v4.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up your API key and import needed things\n",
    "import os, mediacloud.api\n",
    "from importlib.metadata import version\n",
    "from dotenv import load_dotenv\n",
    "import datetime as dt\n",
    "from IPython.display import JSON\n",
    "import bokeh.io\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()\n",
    "MC_API_KEY = 'fd180b0f9ce16f29121870731d39725c24f094e5'\n",
    "search_api = mediacloud.api.SearchApi(MC_API_KEY)\n",
    "f'Using Media Cloud python client v{version(\"mediacloud\")}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying boston globe climate solutions journalism article from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = 'url:\"https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6,10)\n",
    "search_api.story_list(my_query,start_date,end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting fulltext data of the boston globe article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_globe_article = search_api.story('bbe590fcc23edb5d310ef99dd1dbddeb1223ff634588d2043ba18e80789700cf')['text']\n",
    "with open(\"boston_globe_article.txt\", \"w\") as file:\n",
    "    file.write(boston_globe_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "with open('boston_globe_article.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores['compound']\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating for Yale 360 article, but using Newspaper3k to scrape, because it was not found using the search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install newspaper3k\n",
    "#%pip install lxml_html_clean\n",
    "from newspaper import Article\n",
    "\n",
    "url = 'https://e360.yale.edu/features/eric-nost-interview'\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "with open(\"yale_360_article.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(article.title + \"\\n\\n\")\n",
    "    f.write(article.text)\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores['compound']\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating for BBC article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#url = 'https://www.bbc.com/news/articles/ce82p6yq061o' \n",
    "#url = 'https://www.bbc.com/news/articles/c1dekp93l6po'\n",
    "url = 'https://www.bbc.com/news/articles/cvgdyl817p1o'\n",
    "\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "with open(\"bbc_article.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(article.title + \"\\n\\n\")\n",
    "    f.write(article.text)\n",
    "\n",
    "with open('bbc_article.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores['compound']\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that it is not necessarily always true that solutions journalism = positive sentiment and non-solutions journalism = negative content, as we have found a sample of non-solutions journalism that has positive sentiment. But we may need to consider sentiment as a feature when creating our classifier pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularising - extract full text and sentiment analysis as two separate funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = 'url:\"https://www.bbc.com/news/articles/cvgdyl817p1o\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6,10)\n",
    "print(search_api.story_list(my_query,start_date,end_date)[0])\n",
    "\n",
    "my_query = 'url:\"https://e360.yale.edu/features/eric-nost-interview\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6,10)\n",
    "x = search_api.story_list(my_query,start_date,end_date)\n",
    "if len(x[0]) == 0:\n",
    "    print(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from newspaper import Article\n",
    "import datetime as dt\n",
    "import requests\n",
    "\n",
    "def extract_full_text(url, output_file_name):\n",
    "    my_query = f'url:\"{url}\"'\n",
    "    start_date = dt.date(2025, 1, 1)\n",
    "    end_date = dt.date(2025, 6, 10)\n",
    "\n",
    "    article_text = \"\"\n",
    "\n",
    "    try:\n",
    "        result = search_api.story_list(my_query, start_date, end_date)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Search API failed: {e}\")\n",
    "        result = None\n",
    "\n",
    "    if result:\n",
    "        try:\n",
    "            article_text = search_api.story(result['id'])['text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching from Media Cloud: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article_text = article.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing article: {e}\")\n",
    "\n",
    "    out_file = f\"{output_file_name}.txt\"\n",
    "    with open(out_file, \"w\", encoding='utf-8') as file:\n",
    "        file.write(article_text)\n",
    "\n",
    "    print(f\"Article saved to {out_file}\")\n",
    "\n",
    "\n",
    "import difflib\n",
    "\n",
    "def compare_files(file1_path, file2_path):\n",
    "    \"\"\"Compares two text files and prints the differences.\"\"\"\n",
    "    with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
    "        file1_lines = file1.readlines()\n",
    "        file2_lines = file2.readlines()\n",
    "\n",
    "    diff = difflib.unified_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path)\n",
    "\n",
    "    for line in diff:\n",
    "        print(line, end=\"\")\n",
    "\n",
    "#def sentiment_analysis(file):\n",
    "    #VADER - pos/neg/neutral\n",
    "    #textblob - Polarity/Subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying that extract_full_text is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_full_text(\"https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/\",\"boston_globe_article_f\")\n",
    "compare_files('boston_globe_article_f.txt', 'boston_globe_article.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_full_text(\"https://e360.yale.edu/features/eric-nost-interview\",\"yale_360_article_f\")\n",
    "compare_files('yale_360_article_f.txt','yale_360_article.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_full_text(\"https://www.bbc.com/news/articles/cvgdyl817p1o\",\"bbc_article_f\")\n",
    "compare_files('bbc_article_f.txt','bbc_article.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = 'All'\n",
    "start_date = dt.date(2024, 1, 1)\n",
    "end_date = dt.date(2025, 6,10)\n",
    "source_list = search_api.sources(my_query,start_date,end_date)\n",
    "import re\n",
    "for i in source_list:\n",
    "    print(i['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], None)\n"
     ]
    }
   ],
   "source": [
    "my_query = 'url:\"https://www.vox.com/future-perfect/*\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "\n",
    "search_api.story_list(my_query, start_date, end_date)\n",
    "\n",
    "\n",
    "my_query = 'url:\"https://www.theguardian.com/world/series/the-upside/*\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "\n",
    "search_api.story_list(my_query, start_date, end_date)\n",
    "\n",
    "my_query = 'url:\"https://www.hcn.org/topic/what-works/*\"'\n",
    "start_date = dt.date(2021, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "\n",
    "print(search_api.story_list(my_query, start_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import dateutil.parser\n",
    "import datetime as dt\n",
    "\n",
    "def crawl_section_urls(base_url, max_pages=5):\n",
    "    \"\"\"\n",
    "    Crawl article URLs from a section URL.\n",
    "    Supports Vox, Guardian, and HCN based on URL patterns.\n",
    "    max_pages limits pagination depth (adjust as needed).\n",
    "    \"\"\"\n",
    "    article_urls = set()\n",
    "    for page in range(1, max_pages + 1):\n",
    "        # Construct page URL depending on site\n",
    "        if \"vox.com\" in base_url:\n",
    "            # Vox paginates with ?page=2 etc\n",
    "            url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        elif \"theguardian.com\" in base_url:\n",
    "            # Guardian paginates with /page/2 etc\n",
    "            url = f\"{base_url}/page/{page}\" if page > 1 else base_url\n",
    "        elif \"hcn.org\" in base_url:\n",
    "            # HCN uses ?page=2 etc\n",
    "            url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        else:\n",
    "            # Unknown site â€” no pagination\n",
    "            url = base_url\n",
    "\n",
    "        print(f\"Crawling page {page} at {url}\")\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"Failed to load page: {url}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Extract article links based on site\n",
    "        links = []\n",
    "        if \"vox.com\" in base_url:\n",
    "            links = [a['href'] for a in soup.select(\"a[href*='/future-perfect/']\") if a.has_attr('href')]\n",
    "        elif \"theguardian.com\" in base_url:\n",
    "            links = []\n",
    "            links += [a['href'] for a in soup.select(\"a.js-headline-text\")]\n",
    "            links += [a['href'] for a in soup.select(\"a.u-faux-block-link__overlay\")]\n",
    "            links += [a['href'] for a in soup.select(\"div.fc-item__content a[href]\")]\n",
    "            links = list(set(links))\n",
    "        elif \"hcn.org\" in base_url:\n",
    "            links = [a['href'] for a in soup.select(\"h2.entry-title a\")]\n",
    "\n",
    "        # Clean and filter full URLs\n",
    "        for link in links:\n",
    "            if link.startswith('/'):\n",
    "                # Make full URL\n",
    "                if \"vox.com\" in base_url:\n",
    "                    full_url = \"https://www.vox.com\" + link\n",
    "                elif \"theguardian.com\" in base_url:\n",
    "                    full_url = \"https://www.theguardian.com\" + link\n",
    "                elif \"hcn.org\" in base_url:\n",
    "                    full_url = \"https://www.hcn.org\" + link\n",
    "                else:\n",
    "                    full_url = link\n",
    "            else:\n",
    "                full_url = link\n",
    "\n",
    "            article_urls.add(full_url)\n",
    "\n",
    "        # Stop if no new links found on this page (end pagination)\n",
    "        if not links:\n",
    "            break\n",
    "\n",
    "    return sorted(article_urls)\n",
    "\n",
    "def fetch_and_filter_articles(urls, keywords, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Download articles, parse publish date, filter by keywords and date range.\n",
    "    Returns list of dict with title, url, publish_date, and snippet.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            art = Article(url)\n",
    "            art.download()\n",
    "            art.parse()\n",
    "\n",
    "            # Only text-based articles (no video/audio-only)\n",
    "            if len(art.text) < 200:\n",
    "                # Skip if text too short (likely not a full article)\n",
    "                continue\n",
    "\n",
    "            pub_date = art.publish_date\n",
    "            if pub_date is None:\n",
    "                # Try to parse date from meta tags manually\n",
    "                # or skip article if no date\n",
    "                pub_date = None\n",
    "            else:\n",
    "                # Normalize to date only\n",
    "                pub_date = pub_date.date()\n",
    "\n",
    "            # Filter by date\n",
    "            if start_date and pub_date and pub_date < start_date:\n",
    "                continue\n",
    "            if end_date and pub_date and pub_date > end_date:\n",
    "                continue\n",
    "\n",
    "            # Keyword filter in title or text (case-insensitive)\n",
    "            text_lower = art.text.lower()\n",
    "            title_lower = (art.title or \"\").lower()\n",
    "            if not any(kw.lower() in text_lower or kw.lower() in title_lower for kw in keywords):\n",
    "                continue\n",
    "\n",
    "            # Save minimal info\n",
    "            snippet = art.text[:300].replace('\\n', ' ') + '...'\n",
    "            results.append({\n",
    "                \"title\": art.title,\n",
    "                \"url\": url,\n",
    "                \"publish_date\": pub_date,\n",
    "                \"snippet\": snippet\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {url}: {e}\")\n",
    "    print(f\"Found {len(results)} articles\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Your section URLs:\n",
    "vox_section = \"https://www.vox.com/future-perfect\"\n",
    "guardian_section = \"https://www.theguardian.com/world/series/the-upside\"\n",
    "hcn_section = \"https://www.hcn.org/topic/what-works/\"\n",
    "\n",
    "# Keywords to filter by:\n",
    "keywords = [\"climate\", \"climate change\", \"health\", \"healthcare\"]\n",
    "\n",
    "# Date range for filtering articles:\n",
    "start_date = dt.date(2024, 1, 1)\n",
    "end_date = dt.date.today()\n",
    "\n",
    "# Crawl article URLs (limit max_pages to control scraping load)\n",
    "print(\"Crawling Vox articles...\")\n",
    "vox_urls = crawl_section_urls(vox_section, max_pages=5)\n",
    "print(f\"ðŸ”— Vox URLs found: {len(vox_urls)}\")\n",
    "\n",
    "print(\"Crawling Guardian articles...\")\n",
    "guardian_urls = crawl_section_urls(guardian_section, max_pages=5)\n",
    "print(f\"ðŸ”— Guardian URLs found: {len(guardian_urls)}\")\n",
    "\n",
    "print(\"Crawling HCN articles...\")\n",
    "hcn_urls = crawl_section_urls(hcn_section, max_pages=3)\n",
    "print(f\"ðŸ”— HCN URLs found: {len(hcn_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"climate\", \"climate change\"]\n",
    "# Fetch articles and filter by keywords and date\n",
    "print(\"Filtering Vox articles...\")\n",
    "vox_matches = fetch_and_filter_articles(vox_urls, keywords, start_date, end_date)\n",
    "print(\"Filtering Guardian articles...\")\n",
    "guardian_matches = fetch_and_filter_articles(guardian_urls, keywords, start_date, end_date)\n",
    "print(\"Filtering HCN articles...\")\n",
    "hcn_matches = fetch_and_filter_articles(hcn_urls, keywords, start_date, end_date)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Vox matches ===\")\n",
    "for article in vox_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")\n",
    "\n",
    "print(\"\\n=== Guardian matches ===\")\n",
    "for article in guardian_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")\n",
    "\n",
    "print(\"\\n=== HCN matches ===\")\n",
    "for article in hcn_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"health\", \"healthcare\"]\n",
    "# Fetch articles and filter by keywords and date\n",
    "print(\"Filtering Vox articles...\")\n",
    "vox_matches = fetch_and_filter_articles(vox_urls, keywords, start_date, end_date)\n",
    "print(\"Filtering Guardian articles...\")\n",
    "guardian_matches = fetch_and_filter_articles(guardian_urls, keywords, start_date, end_date)\n",
    "print(\"Filtering HCN articles...\")\n",
    "hcn_matches = fetch_and_filter_articles(hcn_urls, keywords, start_date, end_date)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Vox matches ===\")\n",
    "for article in vox_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")\n",
    "\n",
    "print(\"\\n=== Guardian matches ===\")\n",
    "for article in guardian_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")\n",
    "\n",
    "print(\"\\n=== HCN matches ===\")\n",
    "for article in hcn_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = \"https://www.theguardian.com/world/series/the-upside\"\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "# Look for relative URLs starting with /world/series/the-upside/\n",
    "pattern = re.compile(r'^/world/series/the-upside/.*')\n",
    "\n",
    "links = []\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a['href']\n",
    "    if pattern.match(href):\n",
    "        full_url = f\"https://www.theguardian.com{href}\"\n",
    "        links.append(full_url)\n",
    "\n",
    "unique_links = list(set(links))\n",
    "print(f\"Found {len(unique_links)} unique links:\")\n",
    "for link in unique_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import concurrent.futures\n",
    "\n",
    "guardian_urls = [\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/02/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/18/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2022/apr/21/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/07/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/26/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/16/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/25/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jul/09/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/23/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jul/16/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/21/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/12/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/19/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/11/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/15/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/14/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/03/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/12/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2024/aug/11/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/26/all\",\n",
    "]\n",
    "\n",
    "keywords = [\"climate\", \"environment\", \"energy\", \"policy\"]  # Replace with your keywords\n",
    "\n",
    "def check_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text.lower()\n",
    "        title = article.title.lower()\n",
    "        if any(k.lower() in text or k.lower() in title for k in keywords):\n",
    "            return url, article.title\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(check_article, url) for url in guardian_urls]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "print(\"Filtered Guardian matches:\")\n",
    "for url, title in results:\n",
    "    print(f\"- {title}\\n  {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import concurrent.futures\n",
    "\n",
    "guardian_urls = [\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/02/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/18/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2022/apr/21/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/07/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/26/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/16/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/25/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jul/09/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/23/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jul/16/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/21/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/12/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/19/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/11/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/15/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/14/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/03/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/12/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2024/aug/11/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/26/all\",\n",
    "]\n",
    "\n",
    "keywords = [\"health\", \"healthcare\"]  # Replace with your keywords\n",
    "\n",
    "def check_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text.lower()\n",
    "        title = article.title.lower()\n",
    "        if any(k.lower() in text or k.lower() in title for k in keywords):\n",
    "            return url, article.title\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(check_article, url) for url in guardian_urls]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "print(\"Filtered Guardian matches:\")\n",
    "for url, title in results:\n",
    "    print(f\"- {title}\\n  {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
