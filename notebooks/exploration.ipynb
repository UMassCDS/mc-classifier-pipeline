{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using Media Cloud python client v4.4.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up your API key in .env file, and import needed things\n",
    "import os\n",
    "import datetime as dt\n",
    "from importlib.metadata import version\n",
    "\n",
    "import mediacloud.api\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import difflib\n",
    "\n",
    "# Sentiment Analysis\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# URL-related libraries\n",
    "from newspaper import Article\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MC_API_KEY\")\n",
    "search_api = mediacloud.api.SearchApi(api_key)\n",
    "directory_api = mediacloud.api.DirectoryApi(api_key)\n",
    "f\"Using Media Cloud python client v{version('mediacloud')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying boston globe climate solutions journalism article from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'id': 'bbe590fcc23edb5d310ef99dd1dbddeb1223ff634588d2043ba18e80789700cf',\n",
       "   'indexed_date': datetime.datetime(2025, 3, 3, 11, 28, 11, 454314, tzinfo=datetime.timezone.utc),\n",
       "   'language': 'en',\n",
       "   'media_name': 'bostonglobe.com',\n",
       "   'media_url': 'bostonglobe.com',\n",
       "   'publish_date': datetime.date(2025, 3, 3),\n",
       "   'title': 'As warming climate hammers coffee crops, this rare bean may someday be your brew',\n",
       "   'url': 'https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/'}],\n",
       " None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_query = 'url:\"https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "search_api.story_list(my_query, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting fulltext data of the boston globe article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_globe_article = search_api.story(\"bbe590fcc23edb5d310ef99dd1dbddeb1223ff634588d2043ba18e80789700cf\")[\"text\"]\n",
    "with open(\"../data/boston_globe_article.txt\", \"w\") as file:\n",
    "    file.write(boston_globe_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.08\n",
      "Subjectivity: 0.40\n",
      "{'neg': 0.082, 'neu': 0.83, 'pos': 0.088, 'compound': 0.2392}\n",
      "Overall sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/boston_globe_article.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "try:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores[\"compound\"]\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating for Yale 360 article, but using Newspaper3k to scrape, because it was not found using the search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.08\n",
      "Subjectivity: 0.40\n",
      "{'neg': 0.082, 'neu': 0.83, 'pos': 0.088, 'compound': 0.2392}\n",
      "Overall sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "url = \"https://e360.yale.edu/features/eric-nost-interview\"\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "with open(\"../data/yale_360_article.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(article.title + \"\\n\\n\")\n",
    "    f.write(article.text)\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "# nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores[\"compound\"]\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating for BBC article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.05\n",
      "Subjectivity: 0.42\n",
      "{'neg': 0.076, 'neu': 0.797, 'pos': 0.127, 'compound': 0.996}\n",
      "Overall sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# url = 'https://www.bbc.com/news/articles/ce82p6yq061o'\n",
    "# url = 'https://www.bbc.com/news/articles/c1dekp93l6po'\n",
    "url = \"https://www.bbc.com/news/articles/cvgdyl817p1o\"\n",
    "\n",
    "my_query = f'url:\"{url}\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "story_info = search_api.story_list(my_query, start_date, end_date)[0][0]\n",
    "story_id = story_info[\"id\"]\n",
    "\n",
    "bbc_article = search_api.story(story_id)[\"text\"]\n",
    "\n",
    "with open(\"../data/bbc_article.txt\", \"w\") as file:\n",
    "    file.write(bbc_article)\n",
    "\n",
    "with open(\"../data/bbc_article.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "print(f\"Polarity: {sentiment.polarity:.2f}\")\n",
    "\n",
    "print(f\"Subjectivity: {sentiment.subjectivity:.2f}\")\n",
    "\n",
    "\n",
    "# nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Interpret the compound score\n",
    "compound_score = scores[\"compound\"]\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = \"Positive\"\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "\n",
    "print(f\"Overall sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that it is not necessarily always true that solutions journalism = positive sentiment and non-solutions journalism = negative content, as we have found a sample of non-solutions journalism that has positive sentiment. But we may need to consider sentiment as a feature when creating our classifier pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularising - extract full text and sentiment analysis as two separate funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'a4b9491b4f15c523ddd4121978664fd4c7a93d3909f99ba1ecfc9aa45b497675', 'indexed_date': datetime.datetime(2025, 6, 10, 21, 40, 17, 122666, tzinfo=datetime.timezone.utc), 'language': 'en', 'media_name': 'bbc.com', 'media_url': 'bbc.com', 'publish_date': datetime.date(2025, 6, 9), 'title': 'Homeowners warned over green energy scammers', 'url': 'https://www.bbc.com/news/articles/cvgdyl817p1o'}]\n",
      "not found\n"
     ]
    }
   ],
   "source": [
    "my_query = 'url:\"https://www.bbc.com/news/articles/cvgdyl817p1o\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "print(search_api.story_list(my_query, start_date, end_date)[0])\n",
    "\n",
    "my_query = 'url:\"https://e360.yale.edu/features/eric-nost-interview\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "x = search_api.story_list(my_query, start_date, end_date)\n",
    "if len(x[0]) == 0:\n",
    "    print(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_text(url, output_file_name):\n",
    "    my_query = f'url:\"{url}\"'\n",
    "    start_date = dt.date(2025, 1, 1)\n",
    "    end_date = dt.date(2025, 6, 10)\n",
    "\n",
    "    article_text = \"\"\n",
    "\n",
    "    try:\n",
    "        print(search_api.story_list(my_query, start_date, end_date)[0][0])\n",
    "        result = search_api.story_list(my_query, start_date, end_date)[0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Search API failed: {e}\")\n",
    "        result = None\n",
    "\n",
    "    if result:\n",
    "        try:\n",
    "            article_text = search_api.story(result[\"id\"])[\"text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching from Media Cloud: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article_text = article.title + \"\\n\\n\" + article.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching URL: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing article: {e}\")\n",
    "\n",
    "    out_file = f\"{output_file_name}.txt\"\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(article_text)\n",
    "\n",
    "    print(f\"Article saved to {out_file}\")\n",
    "\n",
    "\n",
    "def compare_files(file1_path, file2_path):\n",
    "    \"\"\"Compares two text files and prints the differences.\"\"\"\n",
    "    with open(file1_path, \"r\") as file1, open(file2_path, \"r\") as file2:\n",
    "        file1_lines = file1.readlines()\n",
    "        file2_lines = file2.readlines()\n",
    "\n",
    "    diff = difflib.unified_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path)\n",
    "\n",
    "    for line in diff:\n",
    "        print(line, end=\"\")\n",
    "\n",
    "\n",
    "# def sentiment_analysis(file):\n",
    "# VADER - pos/neg/neutral\n",
    "# textblob - Polarity/Subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying that extract_full_text is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'bbe590fcc23edb5d310ef99dd1dbddeb1223ff634588d2043ba18e80789700cf', 'indexed_date': datetime.datetime(2025, 3, 3, 11, 28, 11, 454314, tzinfo=datetime.timezone.utc), 'language': 'en', 'media_name': 'bostonglobe.com', 'media_url': 'bostonglobe.com', 'publish_date': datetime.date(2025, 3, 3), 'title': 'As warming climate hammers coffee crops, this rare bean may someday be your brew', 'url': 'https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/'}\n",
      "Article saved to ../data/boston_globe_article_f.txt\n"
     ]
    }
   ],
   "source": [
    "extract_full_text(\n",
    "    \"https://www.bostonglobe.com/2025/03/03/world/south-sudan-coffee-beans-climate-change/\",\n",
    "    \"../data/boston_globe_article_f\",\n",
    ")\n",
    "compare_files(\"../data/boston_globe_article_f.txt\", \"../data/boston_globe_article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search API failed: list index out of range\n",
      "Article saved to ../data/yale_360_article_f.txt\n"
     ]
    }
   ],
   "source": [
    "extract_full_text(\"https://e360.yale.edu/features/eric-nost-interview\", \"../data/yale_360_article_f\")\n",
    "compare_files(\"../data/yale_360_article_f.txt\", \"../data/yale_360_article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'a4b9491b4f15c523ddd4121978664fd4c7a93d3909f99ba1ecfc9aa45b497675', 'indexed_date': datetime.datetime(2025, 6, 10, 21, 40, 17, 122666, tzinfo=datetime.timezone.utc), 'language': 'en', 'media_name': 'bbc.com', 'media_url': 'bbc.com', 'publish_date': datetime.date(2025, 6, 9), 'title': 'Homeowners warned over green energy scammers', 'url': 'https://www.bbc.com/news/articles/cvgdyl817p1o'}\n",
      "Article saved to ../data/bbc_article_f.txt\n"
     ]
    }
   ],
   "source": [
    "extract_full_text(\"https://www.bbc.com/news/articles/cvgdyl817p1o\", \"../data/bbc_article_f\")\n",
    "compare_files(\"../data/bbc_article_f.txt\", \"../data/bbc_article.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttblogs.com\n",
      "blogdomago.com\n",
      "amarujala.com\n",
      "yahoo.com\n",
      "indiatimes.com\n",
      "latestnigeriannews.com\n",
      "benzinga.com\n",
      "dailymail.co.uk\n",
      "yardbarker.com\n",
      "365project.org\n",
      "hindustantimes.com\n",
      "mirror.co.uk\n",
      "globenewswire.com\n",
      "thesun.co.uk\n",
      "kdhnews.com\n",
      "sktoday.com\n",
      "noticiasya.com\n",
      "freerepublic.com\n",
      "india.com\n",
      "express.co.uk\n",
      "independent.co.uk\n",
      "finanznachrichten.de\n",
      "prnewswire.com\n",
      "screenrant.com\n",
      "forbes.com\n",
      "wtop.com\n",
      "metroseoul.co.kr\n",
      "wvnews.com\n",
      "cbsnews.com\n",
      "niagarafallsreview.ca\n",
      "wellandtribune.ca\n",
      "thespec.com\n",
      "thehindu.com\n",
      "therecord.com\n",
      "the-messenger.com\n",
      "ctvnews.ca\n",
      "indianexpress.com\n",
      "upstract.com\n",
      "nst.com.my\n",
      "einpresswire.com\n",
      "theguardian.com\n",
      "kesq.com\n",
      "sportingnews.com\n",
      "livemint.com\n",
      "keyt.com\n",
      "gazette.com\n",
      "news18.com\n",
      "ktvz.com\n",
      "dailyrecord.co.uk\n",
      "kion546.com\n",
      "thestar.com\n",
      "abc17news.com\n",
      "independent.ie\n",
      "citynews.ca\n",
      "newsday.com\n",
      "nytimes.com\n",
      "mediaindonesia.com\n",
      "seznam.cz\n",
      "krdo.com\n",
      "nypost.com\n",
      "localnews8.com\n",
      "biztoc.com\n",
      "the-sun.com\n",
      "urdupoint.com\n",
      "winnipegfreepress.com\n",
      "zawya.com\n",
      "newsweek.com\n",
      "cbssports.com\n",
      "regionalmedianews.com\n",
      "manchestereveningnews.co.uk\n",
      "nature.com\n",
      "standard.co.uk\n",
      "marketbeat.com\n",
      "newindianexpress.com\n",
      "freepressjournal.in\n",
      "ndtv.com\n",
      "metro.co.uk\n",
      "perthnow.com.au\n",
      "tribuneindia.com\n",
      "menafn.com\n",
      "sangbadpratidin.in\n",
      "birminghammail.co.uk\n",
      "usatoday.com\n",
      "sportskeeda.com\n",
      "dailystar.co.uk\n",
      "lakelandtoday.ca\n",
      "burnabynow.com\n",
      "townandcountrytoday.com\n",
      "kvia.com\n",
      "richmond-news.com\n",
      "westernwheel.ca\n",
      "stalbertgazette.com\n",
      "tayyar.org\n",
      "piquenewsmagazine.com\n",
      "nsnews.com\n",
      "tricitynews.com\n",
      "thewest.com.au\n",
      "rmoutlook.com\n",
      "bowenislandundercurrent.com\n",
      "prpeak.com\n"
     ]
    }
   ],
   "source": [
    "my_query = \"All\"\n",
    "start_date = dt.date(2024, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "source_list = search_api.sources(my_query, start_date, end_date)\n",
    "for i in source_list:\n",
    "    print(i[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging deeper into sources where only particular sections are SoJo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_query = \\'url:\"https://www.vox.com/future-perfect/*\"\\'\\nstart_date = dt.date(2025, 1, 1)\\nend_date = dt.date(2025, 6, 10)\\n\\nsearch_api.story_list(my_query, start_date, end_date)\\n\\n\\nmy_query = \\'url:\"https://www.theguardian.com/world/series/the-upside/*\"\\'\\nstart_date = dt.date(2025, 1, 1)\\nend_date = dt.date(2025, 6, 10)\\n\\nsearch_api.story_list(my_query, start_date, end_date)\\n\\nmy_query = \\'url:\"https://www.hcn.org/topic/what-works/*\"\\'\\nstart_date = dt.date(2021, 1, 1)\\nend_date = dt.date(2025, 6, 10)\\n\\nmy_query = \\'url:\"https://montanafreepress.org/series/long-streets/*\"\\'\\nstart_date = dt.date(2021, 1, 1)\\nend_date = dt.date(2025, 6, 10)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"my_query = 'url:\"https://www.vox.com/future-perfect/*\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "\n",
    "search_api.story_list(my_query, start_date, end_date)\n",
    "\n",
    "\n",
    "my_query = 'url:\"https://www.theguardian.com/world/series/the-upside/*\"'\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "\n",
    "search_api.story_list(my_query, start_date, end_date)\n",
    "\n",
    "my_query = 'url:\"https://www.hcn.org/topic/what-works/*\"'\n",
    "start_date = dt.date(2021, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\n",
    "\n",
    "my_query = 'url:\"https://montanafreepress.org/series/long-streets/*\"'\n",
    "start_date = dt.date(2021, 1, 1)\n",
    "end_date = dt.date(2025, 6, 10)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_section_urls(base_url, max_pages=5):\n",
    "    \"\"\"\n",
    "    Crawl article URLs from a section URL.\n",
    "    Supports Vox, Guardian, and HCN based on URL patterns.\n",
    "    max_pages limits pagination depth (adjust as needed).\n",
    "    \"\"\"\n",
    "    article_urls = set()\n",
    "    for page in range(1, max_pages + 1):\n",
    "        # Construct page URL depending on site\n",
    "        if \"vox.com\" in base_url:\n",
    "            # Vox paginates with ?page=2 etc\n",
    "            url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        elif \"theguardian.com\" in base_url:\n",
    "            # Guardian paginates with /page/2 etc\n",
    "            url = f\"{base_url}/page/{page}\" if page > 1 else base_url\n",
    "        elif \"hcn.org\" in base_url:\n",
    "            # HCN uses ?page=2 etc\n",
    "            url = f\"{base_url}?page={page}\" if page > 1 else base_url\n",
    "        else:\n",
    "            # Unknown site ‚Äî no pagination\n",
    "            url = base_url\n",
    "\n",
    "        print(f\"Crawling page {page} at {url}\")\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"Failed to load page: {url}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Extract article links based on site\n",
    "        links = []\n",
    "        if \"vox.com\" in base_url:\n",
    "            links = [a[\"href\"] for a in soup.select(\"a[href*='/future-perfect/']\") if a.has_attr(\"href\")]\n",
    "        elif \"theguardian.com\" in base_url:\n",
    "            links = []\n",
    "            links += [a[\"href\"] for a in soup.select(\"a.js-headline-text\")]\n",
    "            links += [a[\"href\"] for a in soup.select(\"a.u-faux-block-link__overlay\")]\n",
    "            links += [a[\"href\"] for a in soup.select(\"div.fc-item__content a[href]\")]\n",
    "            links = list(set(links))\n",
    "        elif \"hcn.org\" in base_url:\n",
    "            links = [a[\"href\"] for a in soup.select(\"h2.entry-title a\")]\n",
    "\n",
    "        # Clean and filter full URLs\n",
    "        for link in links:\n",
    "            if link.startswith(\"/\"):\n",
    "                # Make full URL\n",
    "                if \"vox.com\" in base_url:\n",
    "                    full_url = \"https://www.vox.com\" + link\n",
    "                elif \"theguardian.com\" in base_url:\n",
    "                    full_url = \"https://www.theguardian.com\" + link\n",
    "                elif \"hcn.org\" in base_url:\n",
    "                    full_url = \"https://www.hcn.org\" + link\n",
    "                else:\n",
    "                    full_url = link\n",
    "            else:\n",
    "                full_url = link\n",
    "\n",
    "            article_urls.add(full_url)\n",
    "\n",
    "        # Stop if no new links found on this page (end pagination)\n",
    "        if not links:\n",
    "            break\n",
    "\n",
    "    return sorted(article_urls)\n",
    "\n",
    "\n",
    "def fetch_and_filter_articles(urls, keywords, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Download articles, parse publish date, filter by keywords and date range.\n",
    "    Returns list of dict with title, url, publish_date, and snippet.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            art = Article(url)\n",
    "            art.download()\n",
    "            art.parse()\n",
    "\n",
    "            # Only text-based articles (no video/audio-only)\n",
    "            if len(art.text) < 200:\n",
    "                # Skip if text too short (likely not a full article)\n",
    "                continue\n",
    "\n",
    "            pub_date = art.publish_date\n",
    "            if pub_date is None:\n",
    "                # Try to parse date from meta tags manually\n",
    "                # or skip article if no date\n",
    "                pub_date = None\n",
    "            else:\n",
    "                # Normalize to date only\n",
    "                pub_date = pub_date.date()\n",
    "\n",
    "            # Filter by date\n",
    "            if start_date and pub_date and pub_date < start_date:\n",
    "                continue\n",
    "            if end_date and pub_date and pub_date > end_date:\n",
    "                continue\n",
    "\n",
    "            # Keyword filter in title or text (case-insensitive)\n",
    "            text_lower = art.text.lower()\n",
    "            title_lower = (art.title or \"\").lower()\n",
    "            if not any(kw.lower() in text_lower or kw.lower() in title_lower for kw in keywords):\n",
    "                continue\n",
    "\n",
    "            # Save minimal info\n",
    "            snippet = art.text[:300].replace(\"\\n\", \" \") + \"...\"\n",
    "            results.append({\"title\": art.title, \"url\": url, \"publish_date\": pub_date, \"snippet\": snippet})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {url}: {e}\")\n",
    "    print(f\"Found {len(results)} articles\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling Vox articles...\n",
      "Crawling page 1 at https://www.vox.com/future-perfect\n",
      "Crawling page 2 at https://www.vox.com/future-perfect?page=2\n",
      "Crawling page 3 at https://www.vox.com/future-perfect?page=3\n",
      "Crawling page 4 at https://www.vox.com/future-perfect?page=4\n",
      "Crawling page 5 at https://www.vox.com/future-perfect?page=5\n",
      "üîó Vox URLs found: 36\n",
      "Crawling Guardian articles...\n",
      "Crawling page 1 at https://www.theguardian.com/world/series/the-upside\n",
      "üîó Guardian URLs found: 0\n",
      "Crawling HCN articles...\n",
      "Crawling page 1 at https://www.hcn.org/topic/what-works/\n",
      "Crawling page 2 at https://www.hcn.org/topic/what-works/?page=2\n",
      "Crawling page 3 at https://www.hcn.org/topic/what-works/?page=3\n",
      "üîó HCN URLs found: 20\n",
      "Crawling MFP articles...\n",
      "Crawling page 1 at https://montanafreepress.org/series/long-streets/\n",
      "üîó MFP URLs found: 0\n"
     ]
    }
   ],
   "source": [
    "# Your section URLs:\n",
    "vox_section = \"https://www.vox.com/future-perfect\"\n",
    "guardian_section = \"https://www.theguardian.com/world/series/the-upside\"\n",
    "hcn_section = \"https://www.hcn.org/topic/what-works/\"\n",
    "mfp_section = \"https://montanafreepress.org/series/long-streets/\"\n",
    "\n",
    "# Keywords to filter by:\n",
    "keywords = [\"climate\", \"climate change\", \"health\", \"healthcare\"]\n",
    "\n",
    "# Date range for filtering articles:\n",
    "start_date = dt.date(2024, 1, 1)\n",
    "end_date = dt.date.today()\n",
    "\n",
    "# Crawl article URLs (limit max_pages to control scraping load)\n",
    "print(\"Crawling Vox articles...\")\n",
    "vox_urls = crawl_section_urls(vox_section, max_pages=5)\n",
    "print(f\"üîó Vox URLs found: {len(vox_urls)}\")\n",
    "\n",
    "print(\"Crawling Guardian articles...\")\n",
    "guardian_urls = crawl_section_urls(guardian_section, max_pages=5)\n",
    "print(f\"üîó Guardian URLs found: {len(guardian_urls)}\")\n",
    "\n",
    "print(\"Crawling HCN articles...\")\n",
    "hcn_urls = crawl_section_urls(hcn_section, max_pages=3)\n",
    "print(f\"üîó HCN URLs found: {len(hcn_urls)}\")\n",
    "\n",
    "print(\"Crawling MFP articles...\")\n",
    "mfp_urls = crawl_section_urls(mfp_section, max_pages=10)\n",
    "print(f\"üîó MFP URLs found: {len(mfp_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Vox articles...\n",
      "Found 16 articles\n",
      "Filtering Guardian articles...\n",
      "Found 0 articles\n",
      "Filtering HCN articles...\n",
      "Found 3 articles\n",
      "\n",
      "=== Vox matches ===\n",
      "- The best plant-based meat products, according to a huge blind taste test\n",
      "  https://www.vox.com/future-perfect/411819/best-plant-based-meat-impossible-beyond-gardein-tofurky\n",
      "\n",
      "- How to find a meaningful job: try ‚Äúmoral ambition,‚Äù says Rutger Bregman\n",
      "  https://www.vox.com/future-perfect/412698/rutger-bregman-moral-ambition-meaningful-career-doing-good\n",
      "\n",
      "- How we stretched our aviation system to the brink\n",
      "  https://www.vox.com/future-perfect/413228/plane-crashes-safety-boeing-newark-aviation-system\n",
      "\n",
      "- One chilling forecast of our AI future is getting wide attention. How realistic is it?\n",
      "  https://www.vox.com/future-perfect/414087/artificial-intelligence-openai-ai-2027-china\n",
      "\n",
      "- Bill Gates shows what the end of perpetual philanthropy looks like\n",
      "  https://www.vox.com/future-perfect/414135/bill-gates-foundation-philanthropy-elon-musk-billionaire\n",
      "\n",
      "- The new pope has strong opinions about AI. Good.\n",
      "  https://www.vox.com/future-perfect/414530/pope-leo-ai-artificial-intelligence-catholicism-religion\n",
      "\n",
      "- What will happen to the US when Harvard can‚Äôt be Harvard\n",
      "  https://www.vox.com/future-perfect/414544/trump-administration-harvard-universities-foreign-students-science\n",
      "\n",
      "- Harvard just fired a tenured professor for the first time in 80 years. Good.\n",
      "  https://www.vox.com/future-perfect/414929/harvard-francesco-gino-academic-science-fraud-honesty-research-dan-ariely\n",
      "\n",
      "- These stories could change how you feel about AI\n",
      "  https://www.vox.com/future-perfect/415100/artificial-intelligence-google-deepmind-alphafold-climate-change-medicine\n",
      "\n",
      "- AI can now stalk you with just a single vacation photo\n",
      "  https://www.vox.com/future-perfect/415646/artificial-intelligencer-chatgpt-claude-privacy-surveillance\n",
      "\n",
      "- What drove the tech right‚Äôs ‚Äî and Elon Musk‚Äôs ‚Äî big, failed bet on Trump\n",
      "  https://www.vox.com/future-perfect/416642/elon-musk-donald-trump-tech-right-democrats\n",
      "\n",
      "- The stunning reversal of humanity‚Äôs oldest bias\n",
      "  https://www.vox.com/future-perfect/416809/sexism-girl-preference-sex-ratios-discrimination-ivf\n",
      "\n",
      "- He‚Äôs the godfather of AI. Now, he has a bold new plan to keep us safe from it.\n",
      "  https://www.vox.com/future-perfect/417087/ai-safety-yoshua-bengio-lawzero\n",
      "\n",
      "- AI doesn‚Äôt have to reason to take your job\n",
      "  https://www.vox.com/future-perfect/417325/artificial-intelligence-apple-reasoning-openai-chatgpt\n",
      "\n",
      "- 5 reasons to be grateful for air conditioning\n",
      "  https://www.vox.com/future-perfect/417340/5-reasons-to-be-grateful-for-air-conditioning\n",
      "\n",
      "- A million kids won‚Äôt live to kindergarten because of this disastrous decision\n",
      "  https://www.vox.com/future-perfect/417904/gavi-rfk-jr-child-mortality-vaccines\n",
      "\n",
      "\n",
      "=== Guardian matches ===\n",
      "\n",
      "=== HCN matches ===\n",
      "- A proposed Utah uranium mine gets the Trump treatment\n",
      "  https://www.hcn.org/articles/a-proposed-utah-uranium-mine-gets-the-trump-treatment/\n",
      "\n",
      "- MAGA and the developers are coming for your public lands\n",
      "  https://www.hcn.org/articles/maga-and-the-developers-are-coming-for-your-public-lands/\n",
      "\n",
      "- Our public lands must not be sold\n",
      "  https://www.hcn.org/articles/our-public-lands-must-not-be-sold/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"climate\", \"climate change\"]\n",
    "# Fetch articles and filter by keywords and date\n",
    "print(\"Filtering Vox articles...\")\n",
    "vox_matches = fetch_and_filter_articles(vox_urls, keywords, start_date, end_date)\n",
    "print(\"Filtering Guardian articles...\")\n",
    "guardian_matches = fetch_and_filter_articles(guardian_urls, keywords, start_date, end_date)\n",
    "print(\"Filtering HCN articles...\")\n",
    "hcn_matches = fetch_and_filter_articles(hcn_urls, keywords, start_date, end_date)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Vox matches ===\")\n",
    "for article in vox_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")\n",
    "\n",
    "print(\"\\n=== Guardian matches ===\")\n",
    "for article in guardian_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")\n",
    "\n",
    "print(\"\\n=== HCN matches ===\")\n",
    "for article in hcn_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Vox articles...\n",
      "Found 18 articles\n",
      "Filtering Guardian articles...\n",
      "Found 0 articles\n",
      "Filtering HCN articles...\n",
      "Found 1 articles\n",
      "\n",
      "=== Vox matches ===\n",
      "- You‚Äôre being lied to about protein\n",
      "  https://www.vox.com/future-perfect/410565/protein-muscle-gain-weightlifting-plant-based-vegan\n",
      "\n",
      "- My family has money but doesn‚Äôt give to charity. How do I challenge them without being weird?\n",
      "  https://www.vox.com/future-perfect/410573/family-friends-charity-donations-communication-defensiveness\n",
      "\n",
      "- How to save 400,000 babies a year\n",
      "  https://www.vox.com/future-perfect/412847/neonatal-sepsis-infant-death-foreign-aid-test\n",
      "\n",
      "- The massive stakes of the Trump administration‚Äôs plans to end animal testing\n",
      "  https://www.vox.com/future-perfect/412854/trump-animal-welfare-research-nih-fda-epa\n",
      "\n",
      "- How switching to a flip phone deepened my friendships\n",
      "  https://www.vox.com/future-perfect/413657/iphone-detox-flip-phone-friendships\n",
      "\n",
      "- Bill Gates shows what the end of perpetual philanthropy looks like\n",
      "  https://www.vox.com/future-perfect/414135/bill-gates-foundation-philanthropy-elon-musk-billionaire\n",
      "\n",
      "- 4,000 chicks died in the mail. They expose a darker truth about the meat industry.\n",
      "  https://www.vox.com/future-perfect/414338/chick-usps-animal-transport-meat\n",
      "\n",
      "- What will happen to the US when Harvard can‚Äôt be Harvard\n",
      "  https://www.vox.com/future-perfect/414544/trump-administration-harvard-universities-foreign-students-science\n",
      "\n",
      "- These stories could change how you feel about AI\n",
      "  https://www.vox.com/future-perfect/415100/artificial-intelligence-google-deepmind-alphafold-climate-change-medicine\n",
      "\n",
      "- This overlooked cause of PTSD is only going to get worse\n",
      "  https://www.vox.com/future-perfect/415294/slaughterhouse-meat-workers-ptsd-mental-health\n",
      "\n",
      "- First comes marriage. Then comes a flirtatious colleague.\n",
      "  https://www.vox.com/future-perfect/415740/open-marriage-polyamory-ethical-non-monogamy\n",
      "\n",
      "- The right refuses to take AI seriously\n",
      "  https://www.vox.com/future-perfect/416339/ai-openai-automation-big-beautiful-reconciliation-trump\n",
      "\n",
      "- The one drug RFK Jr. should actually ban\n",
      "  https://www.vox.com/future-perfect/416398/ractopamine-pork-beef-elanco-animal-drug\n",
      "\n",
      "- The one thing the Trump administration got very right\n",
      "  https://www.vox.com/future-perfect/417127/trump-nih-harvard-defunding-monkey-research-livingstone\n",
      "\n",
      "- The bizarre pancreas loophole that‚Äôs undermining America‚Äôs organ donation system\n",
      "  https://www.vox.com/future-perfect/417241/pancreas-loophole-organ-procurement-regulation\n",
      "\n",
      "- ChatGPT and OCD are a dangerous combo\n",
      "  https://www.vox.com/future-perfect/417644/ai-chatgpt-ocd-obsessive-compulsive-disorder-chatbots\n",
      "\n",
      "- Is it even possible to convince people to stop eating meat?\n",
      "  https://www.vox.com/future-perfect/417717/meat-reduction-vegetarian-research\n",
      "\n",
      "- A million kids won‚Äôt live to kindergarten because of this disastrous decision\n",
      "  https://www.vox.com/future-perfect/417904/gavi-rfk-jr-child-mortality-vaccines\n",
      "\n",
      "\n",
      "=== Guardian matches ===\n",
      "\n",
      "=== HCN matches ===\n",
      "- The Fallon Paiute-Shoshone Tribe is bridging Nevada‚Äôs healthcare gap\n",
      "  https://www.hcn.org/articles/the-fallon-paiute-shoshone-tribe-is-bridging-nevadas-healthcare-gap/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"health\", \"healthcare\"]\n",
    "# Fetch articles and filter by keywords and date\n",
    "print(\"Filtering Vox articles...\")\n",
    "vox_matches = fetch_and_filter_articles(vox_urls, keywords, start_date, end_date)\n",
    "print(\"Filtering Guardian articles...\")\n",
    "guardian_matches = fetch_and_filter_articles(guardian_urls, keywords, start_date, end_date)\n",
    "print(\"Filtering HCN articles...\")\n",
    "hcn_matches = fetch_and_filter_articles(hcn_urls, keywords, start_date, end_date)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Vox matches ===\")\n",
    "for article in vox_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")\n",
    "\n",
    "print(\"\\n=== Guardian matches ===\")\n",
    "for article in guardian_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")\n",
    "\n",
    "print(\"\\n=== HCN matches ===\")\n",
    "for article in hcn_matches:\n",
    "    print(f\"- {article['title']}\\n  {article['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 unique links:\n",
      "https://www.theguardian.com/world/series/the-upside/2022/apr/21/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/may/14/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/feb/15/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/jun/18/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/jul/09/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/apr/23/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/mar/19/all\n",
      "https://www.theguardian.com/world/series/the-upside/2024/aug/11/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/may/03/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/jun/11/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/may/21/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/jun/25/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/feb/26/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/may/07/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/apr/02/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/mar/26/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/apr/16/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/feb/12/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/mar/12/all\n",
      "https://www.theguardian.com/world/series/the-upside/2021/jul/16/all\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.theguardian.com/world/series/the-upside\"\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "# Look for relative URLs starting with /world/series/the-upside/\n",
    "pattern = re.compile(r\"^/world/series/the-upside/.*\")\n",
    "\n",
    "links = []\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a[\"href\"]\n",
    "    if pattern.match(href):\n",
    "        full_url = f\"https://www.theguardian.com{href}\"\n",
    "        links.append(full_url)\n",
    "\n",
    "unique_links = list(set(links))\n",
    "print(f\"Found {len(unique_links)} unique links:\")\n",
    "for link in unique_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Guardian matches:\n"
     ]
    }
   ],
   "source": [
    "guardian_urls = [\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/02/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/18/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2022/apr/21/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/07/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/26/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/16/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/25/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jul/09/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/23/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jul/16/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/21/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/12/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/19/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/11/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/15/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/14/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/03/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/12/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2024/aug/11/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/26/all\",\n",
    "]\n",
    "\n",
    "keywords = [\"climate\", \"environment\", \"energy\", \"policy\"]  # Replace with your keywords\n",
    "\n",
    "\n",
    "def check_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text.lower()\n",
    "        title = article.title.lower()\n",
    "        if any(k.lower() in text or k.lower() in title for k in keywords):\n",
    "            return url, article.title\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(check_article, url) for url in guardian_urls]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "print(\"Filtered Guardian matches:\")\n",
    "for url, title in results:\n",
    "    print(f\"- {title}\\n  {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Guardian matches:\n"
     ]
    }
   ],
   "source": [
    "guardian_urls = [\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/02/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/18/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2022/apr/21/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/07/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/26/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/16/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/25/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jul/09/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/apr/23/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jul/16/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/21/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/12/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/19/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/jun/11/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/feb/15/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/14/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/may/03/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/12/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2024/aug/11/all\",\n",
    "    \"https://www.theguardian.com/world/series/the-upside/2021/mar/26/all\",\n",
    "]\n",
    "\n",
    "keywords = [\"health\", \"healthcare\"]  # Replace with your keywords\n",
    "\n",
    "\n",
    "def check_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text.lower()\n",
    "        title = article.title.lower()\n",
    "        if any(k.lower() in text or k.lower() in title for k in keywords):\n",
    "            return url, article.title\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(check_article, url) for url in guardian_urls]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "print(\"Filtered Guardian matches:\")\n",
    "for url, title in results:\n",
    "    print(f\"- {title}\\n  {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dataframe for Sojo sources and articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cmagapu/MediaCloud/mc-classifier-pipeline/notebooks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SoJo source</th>\n",
       "      <th>Homepage URL</th>\n",
       "      <th>Present in Media Cloud (yes/no)</th>\n",
       "      <th>Has health stories (yes/no)</th>\n",
       "      <th>Has climate stories(yes/no)</th>\n",
       "      <th>Notes/Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BBC: ‚ÄúPeople Fixing the World‚Äù</td>\n",
       "      <td>https://www.bbc.co.uk/programmes/p04d42vf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is a podcast/youtube series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBC: ‚ÄúMy Perfect Country‚Äù</td>\n",
       "      <td>https://www.bbc.co.uk/programmes/p03gsc50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is a podcast/youtube series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BBC: ‚ÄúCrossing Divides‚Äù</td>\n",
       "      <td>https://www.bbc.com/news/topics/czpqp1q456vt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is a podcast/youtube series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BBC: ‚ÄúFuture Planet‚Äù</td>\n",
       "      <td>https://www.bbc.co.uk/future/future-planet</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLITICO Magazine: ‚ÄúWhat Works‚Äù + ‚ÄúWhat Works ...</td>\n",
       "      <td>https://www.politico.com/magazine/what-works-n...</td>\n",
       "      <td>No</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         SoJo source  \\\n",
       "0                     BBC: ‚ÄúPeople Fixing the World‚Äù   \n",
       "1                          BBC: ‚ÄúMy Perfect Country‚Äù   \n",
       "2                            BBC: ‚ÄúCrossing Divides‚Äù   \n",
       "3                               BBC: ‚ÄúFuture Planet‚Äù   \n",
       "4  POLITICO Magazine: ‚ÄúWhat Works‚Äù + ‚ÄúWhat Works ...   \n",
       "\n",
       "                                        Homepage URL  \\\n",
       "0          https://www.bbc.co.uk/programmes/p04d42vf   \n",
       "1          https://www.bbc.co.uk/programmes/p03gsc50   \n",
       "2       https://www.bbc.com/news/topics/czpqp1q456vt   \n",
       "3         https://www.bbc.co.uk/future/future-planet   \n",
       "4  https://www.politico.com/magazine/what-works-n...   \n",
       "\n",
       "  Present in Media Cloud (yes/no) Has health stories (yes/no)  \\\n",
       "0                             NaN                         NaN   \n",
       "1                             NaN                         NaN   \n",
       "2                             NaN                         NaN   \n",
       "3                              No                         Yes   \n",
       "4                              No                          no   \n",
       "\n",
       "  Has climate stories(yes/no)                   Notes/Questions  \n",
       "0                         NaN  This is a podcast/youtube series  \n",
       "1                         NaN  This is a podcast/youtube series  \n",
       "2                         NaN  This is a podcast/youtube series  \n",
       "3                         Yes                               NaN  \n",
       "4                          no                               NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "df = pd.read_csv(\"../data/SoJosources.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SoJo source</th>\n",
       "      <th>Homepage URL</th>\n",
       "      <th>Present in Media Cloud (yes/no)</th>\n",
       "      <th>Has health stories (yes/no)</th>\n",
       "      <th>Has climate stories(yes/no)</th>\n",
       "      <th>Notes/Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Seattle Times: ‚ÄúEducation Lab‚Äù + ‚ÄúTraffic ...</td>\n",
       "      <td>https://www.seattletimes.com/education-lab/</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The New York Times: ‚ÄúFixes‚Äù</td>\n",
       "      <td>https://www.nytimes.com/column/fixes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Boston Globe: ‚ÄúThings That Work‚Äù</td>\n",
       "      <td>https://apps.bostonglobe.com/metro/graphics/20...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Montana Free Press: ‚ÄúLong Streets‚Äù</td>\n",
       "      <td>https://montanafreepress.org/</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>although it has health and climate stories, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Milwaukee Journal Sentinel: ‚ÄúWisconsin Ideas Lab‚Äù</td>\n",
       "      <td>https://www.jsonline.com/</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>not sojo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          SoJo source  \\\n",
       "5   The Seattle Times: ‚ÄúEducation Lab‚Äù + ‚ÄúTraffic ...   \n",
       "8                         The New York Times: ‚ÄúFixes‚Äù   \n",
       "10                   Boston Globe: ‚ÄúThings That Work‚Äù   \n",
       "13                 Montana Free Press: ‚ÄúLong Streets‚Äù   \n",
       "16  Milwaukee Journal Sentinel: ‚ÄúWisconsin Ideas Lab‚Äù   \n",
       "\n",
       "                                         Homepage URL  \\\n",
       "5         https://www.seattletimes.com/education-lab/   \n",
       "8                https://www.nytimes.com/column/fixes   \n",
       "10  https://apps.bostonglobe.com/metro/graphics/20...   \n",
       "13                      https://montanafreepress.org/   \n",
       "16                          https://www.jsonline.com/   \n",
       "\n",
       "   Present in Media Cloud (yes/no) Has health stories (yes/no)  \\\n",
       "5                              Yes                         Yes   \n",
       "8                              Yes                         Yes   \n",
       "10                             yes                         yes   \n",
       "13                             yes                         yes   \n",
       "16                             yes                         yes   \n",
       "\n",
       "   Has climate stories(yes/no)  \\\n",
       "5                           No   \n",
       "8                          Yes   \n",
       "10                          no   \n",
       "13                         yes   \n",
       "16                          no   \n",
       "\n",
       "                                      Notes/Questions  \n",
       "5                                                 NaN  \n",
       "8                                                 NaN  \n",
       "10                                                NaN  \n",
       "13  although it has health and climate stories, th...  \n",
       "16                                           not sojo  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_present = df[df[\"Present in Media Cloud (yes/no)\"].str.strip().str.lower().str.contains(\"^yes\", na=False)]\n",
    "\n",
    "df_present.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/1kvy5sks01x02zp7n07cs54h0000gn/T/ipykernel_35018/3071265298.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<IntegerArray>\n",
      "[  24940,       1,    <NA>,  368086,      36,   18482,  712531,  214610,\n",
      "   25249,  370820,       3,   30766,  448086,  745626,  287350,   98379,\n",
      "  538233,  902437, 1402887,  194496,  104828,   98421,  300560,  223331,\n",
      "  711091,    <NA>,   27343,  717756,  278039,   65721,   18014,   28710]\n",
      "Length: 32, dtype: Int64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_present.loc[:, \"source_id\"] = converted_source_id\n"
     ]
    }
   ],
   "source": [
    "df_present = df_present.copy()\n",
    "\n",
    "df_present.loc[:, \"clean_domain\"] = df_present[\"Homepage URL\"].apply(\n",
    "    lambda url: urlparse(url).netloc.replace(\"www.\", \"\") if pd.notnull(url) else None\n",
    ")\n",
    "\n",
    "\n",
    "def get_source_id(domain):\n",
    "    if not domain:\n",
    "        return None\n",
    "    response = directory_api.source_list(name=domain)\n",
    "    if response[\"count\"] > 0:\n",
    "        return response[\"results\"][0].get(\"id\")\n",
    "    return None\n",
    "\n",
    "\n",
    "df_present.loc[:, \"source_id\"] = df_present[\"clean_domain\"].apply(get_source_id)\n",
    "\n",
    "# Convert to numeric with coercion, store in a separate variable\n",
    "converted_source_id = pd.to_numeric(df_present[\"source_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Then assign back the converted series\n",
    "df_present.loc[:, \"source_id\"] = converted_source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SoJo source</th>\n",
       "      <th>Homepage URL</th>\n",
       "      <th>Present in Media Cloud (yes/no)</th>\n",
       "      <th>Has health stories (yes/no)</th>\n",
       "      <th>Has climate stories(yes/no)</th>\n",
       "      <th>Notes/Questions</th>\n",
       "      <th>clean_domain</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Seattle Times: ‚ÄúEducation Lab‚Äù + ‚ÄúTraffic ...</td>\n",
       "      <td>https://www.seattletimes.com/education-lab/</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>seattletimes.com</td>\n",
       "      <td>24940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The New York Times: ‚ÄúFixes‚Äù</td>\n",
       "      <td>https://www.nytimes.com/column/fixes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Boston Globe: ‚ÄúThings That Work‚Äù</td>\n",
       "      <td>https://apps.bostonglobe.com/metro/graphics/20...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apps.bostonglobe.com</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Montana Free Press: ‚ÄúLong Streets‚Äù</td>\n",
       "      <td>https://montanafreepress.org/</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>although it has health and climate stories, th...</td>\n",
       "      <td>montanafreepress.org</td>\n",
       "      <td>368086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Milwaukee Journal Sentinel: ‚ÄúWisconsin Ideas Lab‚Äù</td>\n",
       "      <td>https://www.jsonline.com/</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>not sojo</td>\n",
       "      <td>jsonline.com</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          SoJo source  \\\n",
       "5   The Seattle Times: ‚ÄúEducation Lab‚Äù + ‚ÄúTraffic ...   \n",
       "8                         The New York Times: ‚ÄúFixes‚Äù   \n",
       "10                   Boston Globe: ‚ÄúThings That Work‚Äù   \n",
       "13                 Montana Free Press: ‚ÄúLong Streets‚Äù   \n",
       "16  Milwaukee Journal Sentinel: ‚ÄúWisconsin Ideas Lab‚Äù   \n",
       "\n",
       "                                         Homepage URL  \\\n",
       "5         https://www.seattletimes.com/education-lab/   \n",
       "8                https://www.nytimes.com/column/fixes   \n",
       "10  https://apps.bostonglobe.com/metro/graphics/20...   \n",
       "13                      https://montanafreepress.org/   \n",
       "16                          https://www.jsonline.com/   \n",
       "\n",
       "   Present in Media Cloud (yes/no) Has health stories (yes/no)  \\\n",
       "5                              Yes                         Yes   \n",
       "8                              Yes                         Yes   \n",
       "10                             yes                         yes   \n",
       "13                             yes                         yes   \n",
       "16                             yes                         yes   \n",
       "\n",
       "   Has climate stories(yes/no)  \\\n",
       "5                           No   \n",
       "8                          Yes   \n",
       "10                          no   \n",
       "13                         yes   \n",
       "16                          no   \n",
       "\n",
       "                                      Notes/Questions          clean_domain  \\\n",
       "5                                                 NaN      seattletimes.com   \n",
       "8                                                 NaN           nytimes.com   \n",
       "10                                                NaN  apps.bostonglobe.com   \n",
       "13  although it has health and climate stories, th...  montanafreepress.org   \n",
       "16                                           not sojo          jsonline.com   \n",
       "\n",
       "    source_id  \n",
       "5       24940  \n",
       "8           1  \n",
       "10       <NA>  \n",
       "13     368086  \n",
       "16         36  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_present.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in source_id column: 2\n"
     ]
    }
   ],
   "source": [
    "# directory_api.source_list(name='bostonglobe.com')['results']\n",
    "nan_count = df_present[\"source_id\"].isna().sum()\n",
    "print(f\"Number of NaNs in source_id column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in source_id column: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SoJo source</th>\n",
       "      <th>Homepage URL</th>\n",
       "      <th>Present in Media Cloud (yes/no)</th>\n",
       "      <th>Has health stories (yes/no)</th>\n",
       "      <th>Has climate stories(yes/no)</th>\n",
       "      <th>Notes/Questions</th>\n",
       "      <th>clean_domain</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Boston Globe: ‚ÄúThings That Work‚Äù</td>\n",
       "      <td>https://apps.bostonglobe.com/metro/graphics/20...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apps.bostonglobe.com</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Mongabay: ‚ÄúEnvironment and Her‚Äù</td>\n",
       "      <td>https://india.mongabay.com/series/environment-...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Environment and her is from Mongabay india, bu...</td>\n",
       "      <td>india.mongabay.com</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         SoJo source  \\\n",
       "10  Boston Globe: ‚ÄúThings That Work‚Äù   \n",
       "40   Mongabay: ‚ÄúEnvironment and Her‚Äù   \n",
       "\n",
       "                                         Homepage URL  \\\n",
       "10  https://apps.bostonglobe.com/metro/graphics/20...   \n",
       "40  https://india.mongabay.com/series/environment-...   \n",
       "\n",
       "   Present in Media Cloud (yes/no) Has health stories (yes/no)  \\\n",
       "10                             yes                         yes   \n",
       "40                             yes                          no   \n",
       "\n",
       "   Has climate stories(yes/no)  \\\n",
       "10                          no   \n",
       "40                         yes   \n",
       "\n",
       "                                      Notes/Questions          clean_domain  \\\n",
       "10                                                NaN  apps.bostonglobe.com   \n",
       "40  Environment and her is from Mongabay india, bu...    india.mongabay.com   \n",
       "\n",
       "    source_id  \n",
       "10       <NA>  \n",
       "40       <NA>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nan_rows = df_present[df_present[\"source_id\"].isna()]\n",
    "nan_rows_domain = df_present[df_present[\"clean_domain\"].isna()]\n",
    "print(f\"Number of NaNs in source_id column: {len(nan_rows)}\")\n",
    "display(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SoJo source</th>\n",
       "      <th>Homepage URL</th>\n",
       "      <th>Present in Media Cloud (yes/no)</th>\n",
       "      <th>Has health stories (yes/no)</th>\n",
       "      <th>Has climate stories(yes/no)</th>\n",
       "      <th>Notes/Questions</th>\n",
       "      <th>clean_domain</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Vox: ‚ÄúFuture Perfect‚Äù</td>\n",
       "      <td>https://www.vox.com/future-perfect</td>\n",
       "      <td>yes?</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>my_query = 'url:\"vox.com/future-perfect/*\"' ga...</td>\n",
       "      <td>vox.com</td>\n",
       "      <td>104828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SoJo source                        Homepage URL  \\\n",
       "34  Vox: ‚ÄúFuture Perfect‚Äù  https://www.vox.com/future-perfect   \n",
       "\n",
       "   Present in Media Cloud (yes/no) Has health stories (yes/no)  \\\n",
       "34                            yes?                         yes   \n",
       "\n",
       "   Has climate stories(yes/no)  \\\n",
       "34                         yes   \n",
       "\n",
       "                                      Notes/Questions clean_domain  source_id  \n",
       "34  my_query = 'url:\"vox.com/future-perfect/*\"' ga...      vox.com     104828  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "104828\n"
     ]
    }
   ],
   "source": [
    "row = df_present[df_present[\"SoJo source\"] == \"Vox: ‚ÄúFuture Perfect‚Äù\"]\n",
    "display(row)\n",
    "print(directory_api.source_list(name=\"https://vox.com/future-perfect\")[\"results\"])\n",
    "print(directory_api.source_list(name=\"vox.com\")[\"results\"][0][\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe because india.mongabay.com and apps.bostonglobe.com aren't recognised by the query engine. Also vox and vox future perfect have the same domain, so it's the same source id. This will count all vox articles as SoJo when it's just the future perfect ones. This is a problem we need to solve when attempting to create a collection on Media Cloud for Solutions Journalism (creating child sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying from our Media Cloud collection: https://search.mediacloud.org/collections/262985244/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 622, 'total': 116981}\n"
     ]
    }
   ],
   "source": [
    "my_query = '\"climate change\"'\n",
    "SOJO_COLLECTION = 262985244\n",
    "results = search_api.story_count(my_query, start_date, end_date, collection_ids=[SOJO_COLLECTION])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 262, 'total': 116981}\n"
     ]
    }
   ],
   "source": [
    "my_query = \"healthcare\"\n",
    "SOJO_COLLECTION = 262985244\n",
    "results = search_api.story_count(my_query, start_date, end_date, collection_ids=[SOJO_COLLECTION])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 2905, 'total': 43713}\n"
     ]
    }
   ],
   "source": [
    "my_query = \"healthcare OR health OR diseases OR enrollees OR clinics OR confidential OR medicare OR nursing OR robert OR mental OR credits OR prevention OR enrolled OR clinic OR diagnosis OR disease OR cancer OR medication OR patient OR disabilities OR guidance OR wellbeing\"\n",
    "SOJO_COLLECTION = 262985244\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "end_date = dt.date(2025, 7, 10)\n",
    "results = search_api.story_count(my_query, start_date, end_date, collection_ids=[SOJO_COLLECTION])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
