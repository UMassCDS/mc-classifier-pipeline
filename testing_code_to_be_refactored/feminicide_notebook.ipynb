{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import mediacloud.api\n",
    "import datetime as dt\n",
    "import json\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse, urlunparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "MC_API_KEY = os.getenv(\"MC_API_KEY\")  \n",
    "RAW_ARTICLES_DIR = \"raw_articles_data\"   \n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(RAW_ARTICLES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media Cloud API initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Media Cloud API\n",
    "try:\n",
    "    search_api = mediacloud.api.SearchApi(MC_API_KEY)\n",
    "    print(\"Media Cloud API initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not initialize Media Cloud API. Check your key. Error: {e}.\")\n",
    "    search_api = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "project_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "published_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "queued_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "processed_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "posted_date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "above_threshold",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "model_1_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "model_2_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "normalized_url",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "1f9a5d03-adc6-4637-a330-5074afc59621",
       "rows": [
        [
         "0",
         "38814",
         "128",
         "6",
         "0.8565066987626243",
         "2025-05-06 16:38:33.000000",
         "2025-06-11 12:34:11.918737",
         "2025-06-11 12:34:17.924794",
         "2025-06-11 12:35:09.115747",
         "True",
         null,
         null,
         "newscatcher",
         "https://www.yahoo.com/news/living-parents-until-police-found-163833967.html",
         "http://yahoo.com/news/living-parents-until-police-found-163833967.html"
        ],
        [
         "1",
         "37418",
         "116",
         "2",
         "0.1025058723003643",
         "2025-05-06 12:33:32.000000",
         "2025-06-11 12:34:09.039428",
         "2025-06-11 12:34:12.287421",
         null,
         "False",
         null,
         null,
         "newscatcher",
         "https://lado.mx/noticia.php?id=18130041",
         "http://lado.mx/noticia.php?id=18130041"
        ],
        [
         "2",
         "37419",
         "116",
         "2",
         "0.3768121846476626",
         "2025-05-06 02:29:18.000000",
         "2025-06-11 12:34:09.039428",
         "2025-06-11 12:34:12.287421",
         null,
         "False",
         null,
         null,
         "newscatcher",
         "https://vanguardia.com.mx/show/o-sea-que-no-hay-nueva-musica-confirma-rihanna-tercer-embarazo-de-a-ap-rocky-desde-la-met-gala-LN15819637",
         "http://vanguardia.com.mx/show/o-sea-que-no-hay-nueva-musica-confirma-rihanna-tercer-embarazo-de-a-ap-rocky-desde-la-met-gala-ln15819637"
        ],
        [
         "3",
         "37420",
         "116",
         "2",
         "0.2859018146361041",
         "2025-05-06 11:00:00.000000",
         "2025-06-11 12:34:09.039428",
         "2025-06-11 12:34:12.287421",
         null,
         "False",
         null,
         null,
         "newscatcher",
         "https://heraldodemexico.com.mx/nacional/2025/5/6/mexicanos-reconocen-liderazgo-del-papa-francisco-696639.html",
         "http://heraldodemexico.com.mx/nacional/2025/5/6/mexicanos-reconocen-liderazgo-del-papa-francisco-696639.html"
        ],
        [
         "4",
         "37421",
         "116",
         "2",
         "0.2195981971843145",
         "2025-05-06 16:34:41.000000",
         "2025-06-11 12:34:09.039428",
         "2025-06-11 12:34:12.287421",
         null,
         "False",
         null,
         null,
         "newscatcher",
         "https://www.quien.com/espectaculos/2025/05/06/cuantos-hijos-tiene-rihanna-y-quien-es-su-esposo",
         "http://quien.com/espectaculos/2025/05/06/cuantos-hijos-tiene-rihanna-y-quien-es-su-esposo"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>project_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>model_score</th>\n",
       "      <th>published_date</th>\n",
       "      <th>queued_date</th>\n",
       "      <th>processed_date</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>above_threshold</th>\n",
       "      <th>model_1_score</th>\n",
       "      <th>model_2_score</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>normalized_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38814</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>0.856507</td>\n",
       "      <td>2025-05-06 16:38:33.000000</td>\n",
       "      <td>2025-06-11 12:34:11.918737</td>\n",
       "      <td>2025-06-11 12:34:17.924794</td>\n",
       "      <td>2025-06-11 12:35:09.115747</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>newscatcher</td>\n",
       "      <td>https://www.yahoo.com/news/living-parents-unti...</td>\n",
       "      <td>http://yahoo.com/news/living-parents-until-pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37418</td>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>0.102506</td>\n",
       "      <td>2025-05-06 12:33:32.000000</td>\n",
       "      <td>2025-06-11 12:34:09.039428</td>\n",
       "      <td>2025-06-11 12:34:12.287421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>newscatcher</td>\n",
       "      <td>https://lado.mx/noticia.php?id=18130041</td>\n",
       "      <td>http://lado.mx/noticia.php?id=18130041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37419</td>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>2025-05-06 02:29:18.000000</td>\n",
       "      <td>2025-06-11 12:34:09.039428</td>\n",
       "      <td>2025-06-11 12:34:12.287421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>newscatcher</td>\n",
       "      <td>https://vanguardia.com.mx/show/o-sea-que-no-ha...</td>\n",
       "      <td>http://vanguardia.com.mx/show/o-sea-que-no-hay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37420</td>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>0.285902</td>\n",
       "      <td>2025-05-06 11:00:00.000000</td>\n",
       "      <td>2025-06-11 12:34:09.039428</td>\n",
       "      <td>2025-06-11 12:34:12.287421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>newscatcher</td>\n",
       "      <td>https://heraldodemexico.com.mx/nacional/2025/5...</td>\n",
       "      <td>http://heraldodemexico.com.mx/nacional/2025/5/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37421</td>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>0.219598</td>\n",
       "      <td>2025-05-06 16:34:41.000000</td>\n",
       "      <td>2025-06-11 12:34:09.039428</td>\n",
       "      <td>2025-06-11 12:34:12.287421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>newscatcher</td>\n",
       "      <td>https://www.quien.com/espectaculos/2025/05/06/...</td>\n",
       "      <td>http://quien.com/espectaculos/2025/05/06/cuant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  project_id  model_id  model_score              published_date  \\\n",
       "0  38814         128         6     0.856507  2025-05-06 16:38:33.000000   \n",
       "1  37418         116         2     0.102506  2025-05-06 12:33:32.000000   \n",
       "2  37419         116         2     0.376812  2025-05-06 02:29:18.000000   \n",
       "3  37420         116         2     0.285902  2025-05-06 11:00:00.000000   \n",
       "4  37421         116         2     0.219598  2025-05-06 16:34:41.000000   \n",
       "\n",
       "                  queued_date              processed_date  \\\n",
       "0  2025-06-11 12:34:11.918737  2025-06-11 12:34:17.924794   \n",
       "1  2025-06-11 12:34:09.039428  2025-06-11 12:34:12.287421   \n",
       "2  2025-06-11 12:34:09.039428  2025-06-11 12:34:12.287421   \n",
       "3  2025-06-11 12:34:09.039428  2025-06-11 12:34:12.287421   \n",
       "4  2025-06-11 12:34:09.039428  2025-06-11 12:34:12.287421   \n",
       "\n",
       "                  posted_date  above_threshold  model_1_score  model_2_score  \\\n",
       "0  2025-06-11 12:35:09.115747             True            NaN            NaN   \n",
       "1                         NaN            False            NaN            NaN   \n",
       "2                         NaN            False            NaN            NaN   \n",
       "3                         NaN            False            NaN            NaN   \n",
       "4                         NaN            False            NaN            NaN   \n",
       "\n",
       "        source                                                url  \\\n",
       "0  newscatcher  https://www.yahoo.com/news/living-parents-unti...   \n",
       "1  newscatcher            https://lado.mx/noticia.php?id=18130041   \n",
       "2  newscatcher  https://vanguardia.com.mx/show/o-sea-que-no-ha...   \n",
       "3  newscatcher  https://heraldodemexico.com.mx/nacional/2025/5...   \n",
       "4  newscatcher  https://www.quien.com/espectaculos/2025/05/06/...   \n",
       "\n",
       "                                      normalized_url  \n",
       "0  http://yahoo.com/news/living-parents-until-pol...  \n",
       "1             http://lado.mx/noticia.php?id=18130041  \n",
       "2  http://vanguardia.com.mx/show/o-sea-que-no-hay...  \n",
       "3  http://heraldodemexico.com.mx/nacional/2025/5/...  \n",
       "4  http://quien.com/espectaculos/2025/05/06/cuant...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/cn-gmmp-story-pull-v1.csv')\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs to fetch: 2985\n"
     ]
    }
   ],
   "source": [
    "mediacloud_df = df[df['source'] == 'media-cloud']\n",
    "urls_to_fetch = mediacloud_df['url'].tolist()\n",
    "\n",
    "print(f\"Total URLs to fetch: {len(urls_to_fetch)}\")\n",
    "\n",
    "mediacloud_df.head()\n",
    "\n",
    "# save as csv\n",
    "mediacloud_df.to_csv('data/cn-gmmp-mediacloud-urls.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize URL for consistent comparison by:\n",
    "    - Converting to lowercase\n",
    "    - Removing trailing slashes\n",
    "    - Removing common tracking parameters\n",
    "    - Standardizing protocol\n",
    "    \"\"\"\n",
    "    # Parse the URL\n",
    "    parsed = urlparse(url.lower())\n",
    "    \n",
    "    # Remove common tracking parameters\n",
    "    tracking_params = {\n",
    "        'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',\n",
    "        'fbclid', 'gclid', 'ref', 'source', 'campaign_id', '_ga', 'mc_cid'\n",
    "    }\n",
    "    \n",
    "    # Split query parameters and filter out tracking ones\n",
    "    if parsed.query:\n",
    "        query_pairs = [pair for pair in parsed.query.split('&') \n",
    "                      if pair.split('=')[0] not in tracking_params]\n",
    "        clean_query = '&'.join(query_pairs) if query_pairs else ''\n",
    "    else:\n",
    "        clean_query = ''\n",
    "    \n",
    "    # Remove trailing slash from path\n",
    "    clean_path = parsed.path.rstrip('/')\n",
    "    \n",
    "    # Reconstruct URL\n",
    "    normalized = urlunparse((\n",
    "        parsed.scheme or 'https',  # Default to https if no scheme\n",
    "        parsed.netloc,\n",
    "        clean_path,\n",
    "        parsed.params,\n",
    "        clean_query,\n",
    "        ''  # Remove fragment\n",
    "    ))\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def get_url_hash(url: str) -> str:\n",
    "    \"\"\"Generate consistent hash for normalized URL.\"\"\"\n",
    "    normalized_url = normalize_url(url)\n",
    "    return hashlib.sha256(normalized_url.encode('utf-8')).hexdigest()\n",
    "\n",
    "def load_processed_urls() -> dict:\n",
    "    \"\"\"\n",
    "    Load all processed URLs from existing files into memory for fast lookup.\n",
    "    Returns dict mapping normalized URLs to their file info.\n",
    "    \"\"\"\n",
    "    processed_urls = {}\n",
    "    \n",
    "    if not os.path.exists(RAW_ARTICLES_DIR):\n",
    "        os.makedirs(RAW_ARTICLES_DIR)\n",
    "        return processed_urls\n",
    "    \n",
    "    print(\"Loading already processed URLs...\")\n",
    "    for filename in tqdm(os.listdir(RAW_ARTICLES_DIR), desc=\"Scanning files\"):\n",
    "        if not filename.endswith('.json'):\n",
    "            continue\n",
    "            \n",
    "        filepath = os.path.join(RAW_ARTICLES_DIR, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                original_url = data.get('url', '')\n",
    "                if original_url:\n",
    "                    normalized_url = normalize_url(original_url)\n",
    "                    processed_urls[normalized_url] = {\n",
    "                        'filename': filename,\n",
    "                        'filepath': filepath,\n",
    "                        'original_url': original_url,\n",
    "                        'status': data.get('status', 'unknown'),\n",
    "                        'retrieved_at': data.get('retrieved_at', ''),\n",
    "                        'has_text': bool(data.get('text', '').strip())\n",
    "                    }\n",
    "        except (json.JSONDecodeError, KeyError, Exception) as e:\n",
    "            print(f\"Warning: Could not read {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len(processed_urls)} already processed URLs\")\n",
    "    return processed_urls\n",
    "\n",
    "def identify_failed_urls(processed_urls: dict) -> list:\n",
    "    \"\"\"\n",
    "    Identify URLs that failed processing or have no text content.\n",
    "    Returns list of URLs that could be reprocessed.\n",
    "    \"\"\"\n",
    "    failed_urls = []\n",
    "    for normalized_url, url_info in processed_urls.items():\n",
    "        # Check if previous attempt failed or has no text\n",
    "        if not url_info['has_text']:\n",
    "            failed_urls.append(url_info['original_url'])\n",
    "            continue\n",
    "        \n",
    "        # Check if status indicates failure\n",
    "        status = url_info.get('status', '')\n",
    "        if status.startswith('failed_') or status == 'not_attempted':\n",
    "            failed_urls.append(url_info['original_url'])\n",
    "    \n",
    "    return failed_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading already processed URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning files: 100%|██████████| 1665/1665 [00:00<00:00, 13887.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1665 already processed URLs\n",
      "Found 0 failed URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_article_text(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the full text of an article from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url: The URL of the article to fetch.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the URL, fetched text, and source of the text.\n",
    "    \"\"\"\n",
    "    article_text = \"\"\n",
    "    source = \"\"\n",
    "    original_url = url\n",
    "    article_title = \"\"\n",
    "    status = \"not_attempted\"\n",
    "\n",
    "    if search_api:\n",
    "        # 'url:https\\://www.vox.com/future-perfect/*'\n",
    "        # mc_search.story_count('url:https\\://www.vox.com/future-perfect/*', \n",
    "        # dt.date(2020,1,1), dt.date(2025,8,1), source_ids=[104828])\n",
    "        # replace : with \\:\n",
    "        escaped_url = original_url.replace(':', '\\:')\n",
    "        # if not escaped_url.endswith('/'):\n",
    "        #      escaped_url += '/'\n",
    "        # escaped_url += '*' # Add wildcard to match sub-paths\n",
    "        \n",
    "        my_query = f'url:{escaped_url}'\n",
    "        start_date = dt.date(2025, 1, 1)\n",
    "        end_date = dt.date.today()\n",
    "        try:\n",
    "            results = search_api.story_list(my_query, start_date, end_date)\n",
    "            # print(results)\n",
    "            if results and len(results[0]) > 0:\n",
    "                story_id = results[0][0]['id']\n",
    "                article_data = search_api.story(story_id)\n",
    "                article_title = article_data.get('title', '')\n",
    "                article_text = article_data.get('text', '')\n",
    "                source = \"mediacloud\"\n",
    "                status = \"success\"\n",
    "                # print(f\"Successfully fetched from Media Cloud: {url}\")\n",
    "            else:\n",
    "                status = \"failed_mediacloud_no_results\"\n",
    "        except Exception as e:\n",
    "            status = f\"failed_mediacloud_exception: {type(e).__name__}: {e}\"\n",
    "\n",
    "    # If no text was retrieved, update status to reflect overall failure\n",
    "    if not article_text:\n",
    "        status = status if status.startswith(\"failed_mediacloud\") else \"failed_no_text_retrieved\"\n",
    "\n",
    "    return {\n",
    "        \"url\": original_url,\n",
    "        \"title\": article_title,\n",
    "        \"text\": article_text,\n",
    "        \"source\": source,\n",
    "        \"retrieved_at\": dt.datetime.now().isoformat(),\n",
    "        \"status\": status,\n",
    "        \"normalized_url\": normalize_url(original_url)  \n",
    "    }\n",
    "\n",
    "def fetch_and_save_articles(urls: list, force_reprocess: bool = False):\n",
    "    \"\"\"\n",
    "    Processes a list of URLs, fetches their text, and saves each to a unique file.\n",
    "    Enhanced with better duplicate detection.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of URLs to process\n",
    "        force_reprocess: If True, reprocess URLs even if they were previously successful\n",
    "    \"\"\"\n",
    "    # Load already processed URLs\n",
    "    processed_urls = load_processed_urls()\n",
    "    \n",
    "    failed_urls_tracker = []\n",
    "    skipped_urls_tracker = []\n",
    "    \n",
    "    # Remove duplicates from input while preserving order\n",
    "    unique_urls = []\n",
    "    seen_normalized = set()\n",
    "    for url in urls:\n",
    "        normalized = normalize_url(url)\n",
    "        if normalized not in seen_normalized:\n",
    "            unique_urls.append(url)\n",
    "            seen_normalized.add(normalized)\n",
    "        else:\n",
    "            print(f\"Duplicate URL in input list (skipped): {url}\")\n",
    "    \n",
    "    print(f\"Processing {len(unique_urls)} unique URLs (removed {len(urls) - len(unique_urls)} duplicates)\")\n",
    "    \n",
    "    for url in tqdm(unique_urls, desc=\"Fetching Articles\", unit=\"article\"):\n",
    "        normalized_url = normalize_url(url)\n",
    "        url_hash = get_url_hash(url)\n",
    "        filename = url_hash + \".json\"\n",
    "        filepath = os.path.join(RAW_ARTICLES_DIR, filename)\n",
    "        \n",
    "        # Check if URL was already processed successfully\n",
    "        if normalized_url in processed_urls and not force_reprocess:\n",
    "            existing_info = processed_urls[normalized_url]\n",
    "            # Only skip if it was successful (has text and good status)\n",
    "            if existing_info['has_text'] and not existing_info['status'].startswith('failed_'):\n",
    "                # print(f\"Skipping already processed URL: {url}\")\n",
    "                skipped_urls_tracker.append(url)\n",
    "                continue\n",
    "        \n",
    "        # If force_reprocess is True, remove existing file\n",
    "        if force_reprocess and normalized_url in processed_urls:\n",
    "            existing_filepath = processed_urls[normalized_url]['filepath']\n",
    "            if os.path.exists(existing_filepath):\n",
    "                os.remove(existing_filepath)\n",
    "                print(f\"Removed existing file for reprocessing: {url}\")\n",
    "        \n",
    "        # Fetch article data\n",
    "        article_data = get_article_text(url)\n",
    "        \n",
    "        if article_data[\"text\"]:\n",
    "            try:\n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(article_data, f, ensure_ascii=False, indent=4)\n",
    "                # print(f\"Saved: {url}\")\n",
    "            except Exception as e:\n",
    "                # Handle file write errors as a failure\n",
    "                failed_urls_tracker.append({\n",
    "                    \"url\": url, \n",
    "                    \"reason\": f\"file_save_error: {type(e).__name__}: {e}\", \n",
    "                    \"status\": article_data.get(\"status\", \"unknown\")\n",
    "                })\n",
    "        else:\n",
    "            # If no text was retrieved, consider it a failure for tracking\n",
    "            failed_urls_tracker.append({\n",
    "                \"url\": url, \n",
    "                \"reason\": article_data.get(\"status\", \"no_text_retrieved\")\n",
    "            })\n",
    "            \n",
    "            # Append to failed_urls.txt incrementally\n",
    "            with open('failed_urls.txt', 'a', encoding='utf-8') as f:\n",
    "                f.write(url + '\\n')\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  Successfully processed: {len(unique_urls) - len(failed_urls_tracker) - len(skipped_urls_tracker)}\")\n",
    "    print(f\"  Skipped (already processed): {len(skipped_urls_tracker)}\")\n",
    "    print(f\"  Failed: {len(failed_urls_tracker)}\")\n",
    "    \n",
    "    if failed_urls_tracker:\n",
    "        print(f\"\\nFailed URLs:\")\n",
    "        for failed in failed_urls_tracker[:5]:  # Show first 5\n",
    "            print(f\"  {failed['url']}: {failed['reason']}\")\n",
    "        if len(failed_urls_tracker) > 5:\n",
    "            print(f\"  ... and {len(failed_urls_tracker) - 5} more\")\n",
    "\n",
    "\n",
    "def reprocess_failed_urls():\n",
    "    \"\"\"\n",
    "    Identify and reprocess URLs that previously failed or have no content.\n",
    "    \"\"\"\n",
    "    print(\"Identifying failed URLs...\")\n",
    "    processed_urls = load_processed_urls()\n",
    "    failed_urls = identify_failed_urls(processed_urls)\n",
    "    \n",
    "    if not failed_urls:\n",
    "        print(\"No failed URLs found to reprocess.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(failed_urls)} failed URLs to reprocess.\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"Examples of failed URLs:\")\n",
    "    for i, url in enumerate(failed_urls[:5]):\n",
    "        normalized = normalize_url(url)\n",
    "        info = processed_urls[normalized]\n",
    "        print(f\"  {url}\")\n",
    "        print(f\"    Status: {info['status']}, Has text: {info['has_text']}\")\n",
    "    \n",
    "    if len(failed_urls) > 5:\n",
    "        print(f\"  ... and {len(failed_urls) - 5} more\")\n",
    "    \n",
    "    # Ask for confirmation\n",
    "    response = input(f\"\\nReprocess {len(failed_urls)} failed URLs? (y/N): \")\n",
    "    if response.lower() != 'y':\n",
    "        print(\"Reprocessing cancelled.\")\n",
    "        return\n",
    "    \n",
    "    # Reprocess with force flag\n",
    "    fetch_and_save_articles(failed_urls, force_reprocess=True)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Normal processing (skips already successful URLs)\n",
    "# fetch_and_save_articles(['https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/'])\n",
    "\n",
    "# Force reprocess specific URLs\n",
    "# fetch_and_save_articles(['https://example.com/article'], force_reprocess=True)\n",
    "\n",
    "# Identify and reprocess all failed URLs interactively\n",
    "# reprocess_failed_urls()\n",
    "\n",
    "# Get list of failed URLs programmatically\n",
    "# processed_urls = load_processed_urls()\n",
    "# failed_urls = identify_failed_urls(processed_urls)\n",
    "# print(f\"Found {len(failed_urls)} failed URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Article Fetching ---\n",
      "Loading already processed URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning files: 100%|██████████| 1664/1664 [00:00<00:00, 4926.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1664 already processed URLs\n",
      "Processing 2985 unique URLs (removed 0 duplicates)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:   0%|          | 5/2985 [00:05<55:10,  1.11s/article]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Article Fetching ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfetch_and_save_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls_to_fetch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Article Fetching Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 111\u001b[0m, in \u001b[0;36mfetch_and_save_articles\u001b[0;34m(urls, force_reprocess)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved existing file for reprocessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Fetch article data\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m article_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_article_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m article_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[47], line 31\u001b[0m, in \u001b[0;36mget_article_text\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     29\u001b[0m end_date \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mtoday()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstory_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# print(results)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mediacloud/api.py:176\u001b[0m, in \u001b[0;36mSearchApi.story_list\u001b[0;34m(self, query, start_date, end_date, collection_ids, source_ids, platform, expanded, pagination_token, sort_order, page_size)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m page_size:\n\u001b[1;32m    175\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m page_size\n\u001b[0;32m--> 176\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearch/story-list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dates_str2objects(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstories\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstories\u001b[39m\u001b[38;5;124m'\u001b[39m], results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpagination_token\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mediacloud/api.py:59\u001b[0m, in \u001b[0;36mBaseApi._query\u001b[0;34m(self, endpoint, params, method)\u001b[0m\n\u001b[1;32m     57\u001b[0m endpoint_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_API_URL \u001b[38;5;241m+\u001b[39m endpoint\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     61\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mpost(endpoint_url, json\u001b[38;5;241m=\u001b[39mparams, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTIMEOUT_SECS)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1386\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Article Fetching ---\")\n",
    "fetch_and_save_articles(urls_to_fetch)\n",
    "print(\"\\n--- Article Fetching Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading already processed URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning files: 100%|██████████| 1665/1665 [00:00<00:00, 11279.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1665 already processed URLs\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary\n",
      "Duplicate URL in input list (skipped): https://www.kgun9.com/news/community-inspired-journalism/midtown-news/woman-73-killed-in-domestic-violence-dispute-on-northside\n",
      "Duplicate URL in input list (skipped): https://www.nytimes.com/2025/05/06/briefing/india-pakistan-strikes-supreme-court-ruling.html\n",
      "Duplicate URL in input list (skipped): https://www.eonline.com/news/1417205/buster-murdaugh-alex-murdaugh-son-marries-brooklynn-white?cmpid=rss-syndicate-genericrss-us-top_stories\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary\n",
      "Duplicate URL in input list (skipped): https://www.kgun9.com/news/community-inspired-journalism/midtown-news/woman-73-killed-in-domestic-violence-dispute-on-northside\n",
      "Duplicate URL in input list (skipped): https://www.nytimes.com/2025/05/06/briefing/india-pakistan-strikes-supreme-court-ruling.html\n",
      "Duplicate URL in input list (skipped): https://www.eonline.com/news/1417205/buster-murdaugh-alex-murdaugh-son-marries-brooklynn-white?cmpid=rss-syndicate-genericrss-us-top_stories\n",
      "Duplicate URL in input list (skipped): https://www.nj.com/bergen/2025/05/ex-cop-who-brutally-stabbed-his-nj-girlfriend-to-death-tries-to-get-life-sentence-overturned.html\n",
      "Duplicate URL in input list (skipped): https://www.postandcourier.com/aikenstandard/entertainment/today-s-events-for-may-7/article_107c58d8-20f2-4654-bc52-48492dc9347e.html\n",
      "Duplicate URL in input list (skipped): https://www.pennlive.com/crime/2025/05/woman-laughs-as-she-is-sent-to-jail-for-shooting-at-house-with-her-child-inside.html\n",
      "Duplicate URL in input list (skipped): https://www.mlive.com/news/flint/2025/05/woman-jailed-after-stabbing-boyfriend-with-steak-knife-in-clio-police-say.html\n",
      "Duplicate URL in input list (skipped): https://www.ksfr.org/npr-news/2025-05-06/police-found-a-missing-woman-60-years-after-she-disappeared-she-wants-to-stay-hidden\n",
      "Duplicate URL in input list (skipped): https://www.knkx.org/2025-05-06/police-found-a-missing-woman-60-years-after-she-disappeared-she-wants-to-stay-hidden\n",
      "Duplicate URL in input list (skipped): https://www.wesh.com/article/death-mount-dora-woman-homicide/64692188\n",
      "Duplicate URL in input list (skipped): https://www.robesonian.com/news/318624/lumbee-tribal-chairman-seeks-support-for-domestic-violence-situations\n",
      "Duplicate URL in input list (skipped): https://theconversation.com/buddhas-foster-mother-played-a-key-role-in-the-orphaned-princes-life-and-is-a-model-for-buddhists-on-mothers-day-255368\n",
      "Duplicate URL in input list (skipped): https://www.laurinburgexchange.com/news/289688/pembroke-woman-charged-with-stabbing-at-laurinburg-apartment\n",
      "Duplicate URL in input list (skipped): https://www.nytimes.com/2025/05/06/us/trump-transgender-athletes-fencing-olympics.html\n",
      "Duplicate URL in input list (skipped): https://people.com/alex-murdaughs-son-buster-murdaugh-marries-brooklynn-white-11729313\n",
      "Duplicate URL in input list (skipped): https://people.com/louisiana-woman-murdered-boyfriends-daughter-6-left-body-bucket-moms-lawn-11729057\n",
      "Duplicate URL in input list (skipped): https://www.wptv.com/news/treasure-coast/region-indian-river-county/indian-river-county-man-70-facing-solicitation-charges-in-murder-for-hire-plot\n",
      "Duplicate URL in input list (skipped): https://www.kpbs.org/news/national/2025/05/06/police-found-a-missing-woman-60-years-after-she-disappeared-she-wants-to-stay-hidden\n",
      "Duplicate URL in input list (skipped): https://www.mlive.com/news/muskegon/2025/05/mason-county-man-charged-with-attempted-murder-after-stabbing.html\n",
      "Duplicate URL in input list (skipped): https://people.com/met-gala-2025-rihanna-ciara-reunite-fourteen-years-after-epic-twitter-feud-11729187\n",
      "Duplicate URL in input list (skipped): https://abcnews.go.com/US/mohsen-mahdawi-columbia-student-freed-ice-feared-citizenship/story?id=121485222\n",
      "Duplicate URL in input list (skipped): https://chicago.suntimes.com/outdoors/2025/05/06/chicago-fishing-lakefront-variety-and-spring-signs-lilacs-and-mothers-day\n",
      "Duplicate URL in input list (skipped): https://slate.com/advice/2025/05/friend-advice-gift-food-ingredients.html?via=rss\n",
      "Duplicate URL in input list (skipped): https://www.insidenova.com/lifestyles/entertainment/the-lincoln-lawyer-emmanuelle-chriqui-jason-o-mara-more-join-season-4/article_a7bd96c6-4d45-5b6c-bf49-6fa44a559603.html\n",
      "Duplicate URL in input list (skipped): https://abcnews.go.com/US/fbi-opened-250-investigations-tied-violent-online-network/story?id=121480884\n",
      "Duplicate URL in input list (skipped): https://www.insidenova.com/lifestyles/health/marijuana-use-while-pregnant-linked-to-preterm-birth-low-birth-weight/article_a85210b1-a0bd-5bfa-aa1c-ebf9f0d5bcff.html\n",
      "Duplicate URL in input list (skipped): https://people.com/bombshells-warhols-muses-laurence-leamer-11729082\n",
      "Duplicate URL in input list (skipped): https://www.theroot.com/guess-what-monique-had-to-say-about-shannon-sharpes-sex-1851779321\n",
      "Duplicate URL in input list (skipped): https://slate.com/news-and-politics/2025/05/why-black-women-arent-protesting-trump-this-time.html?via=rss\n",
      "Duplicate URL in input list (skipped): https://www.krwg.org/national-news/2025-05-06/5-new-books-to-check-out-this-week-including-isabel-allendes-latest\n",
      "Duplicate URL in input list (skipped): https://www.newyorker.com/magazine/the-sporting-scene/the-spectacle-of-a-boxing-match-in-times-square\n",
      "Duplicate URL in input list (skipped): https://www.krwg.org/national-news/2025-05-06/syrian-druze-recall-what-they-believe-to-be-past-lives-even-vivid-details\n",
      "Duplicate URL in input list (skipped): https://www.ivpressonline.com/news/mexicali-briefs-presumed-murder-suicide-case-reported/article_a6d1852e-e44f-4db4-a8a2-17f354523c78.html\n",
      "Duplicate URL in input list (skipped): https://ny1.com/nyc/all-boroughs/news/2025/05/06/met-gala-2025-new-york-city\n",
      "Duplicate URL in input list (skipped): https://www.inforum.com/news/north-dakota/elderly-aneta-woman-killed-in-head-on-collision-while-driving-the-wrong-way-on-i-29\n",
      "Processing 1321 unique URLs (removed 47 duplicates)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:   0%|          | 0/1321 [00:00<?, ?article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing file for reprocessing: https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles: 100%|██████████| 1321/1321 [20:59<00:00,  1.05article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "  Successfully processed: 1309\n",
      "  Skipped (already processed): 0\n",
      "  Failed: 12\n",
      "\n",
      "Failed URLs:\n",
      "  https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary/: failed_mediacloud_no_results\n",
      "  https://radaronline.com/p/sydney-sweeney-engaged-patrick-schwarzenegger-intimate-moments: failed_mediacloud_no_results\n",
      "  https://www.nbcchicago.com/news/local/surreal-family-friends-remember-suburban-teen-killed-in-crash-after-senior-prom/3738924: failed_mediacloud_no_results\n",
      "  https://radaronline.com/p/bill-murray-blasted-hollywood-grouchy-poor-me-behavior: failed_mediacloud_no_results\n",
      "  https://www.thenation.com/article/society/joshua-clover-obituary-uc-davis: failed_mediacloud_no_results\n",
      "  ... and 7 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('failed_urls.txt', 'r', encoding='utf-8') as f:\n",
    "    failed_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "fetch_and_save_articles(failed_urls, force_reprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original CSV...\n",
      "Loaded 2985 rows from CSV\n",
      "Loading 2973 article files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading articles: 100%|██████████| 2973/2973 [00:00<00:00, 5343.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2973 article files\n",
      "Merging article data with CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging data: 100%|██████████| 2985/2985 [00:00<00:00, 6696.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging Statistics:\n",
      "  Total rows: 2985\n",
      "  Matched with article data: 2865\n",
      "  Unmatched: 120\n",
      "  Rows with article text: 2865\n",
      "\n",
      "First 5 unmatched URLs:\n",
      "  https://chicago.suntimes.com/obituaries/2025/05/06/michael-miner-dead-media-columnist-chicago-reader-obituary\n",
      "  https://hotspotsmagazine.com/2025/05/06/queer-music-trailblazer-jill-sobule-dies-at-66-in-house-fire/?utm_source=rss&utm_medium=rss&utm_campaign=queer-music-trailblazer-jill-sobule-dies-at-66-in-house-fire\n",
      "  https://www.seattletimes.com/entertainment/singer-songwriter-lizzy-mcalpine-makes-her-broadway-debut-coming-at-a-perfect-time/?utm_source=RSS&utm_medium=Referral&utm_campaign=RSS_all\n",
      "  https://www.seattletimes.com/nation-world/nation/a-philadelphia-woman-is-the-8th-person-to-die-from-the-january-crash-of-a-medical-plane/?utm_source=RSS&utm_medium=Referral&utm_campaign=RSS_all\n",
      "  https://www.seattletimes.com/entertainment/movies/siff-2025-seattles-premier-film-festival-returns-in-challenging-times/?utm_source=RSS&utm_medium=Referral&utm_campaign=RSS_all\n",
      "  ... and 115 more\n",
      "\n",
      "Saving merged data to merged_articles.csv...\n",
      "Saved successfully!\n",
      "\n",
      "==================================================\n",
      "MERGE ANALYSIS\n",
      "==================================================\n",
      "Total articles: 2985\n",
      "With text: 2865 (96.0%)\n",
      "Without text: 120 (4.0%)\n",
      "\n",
      "Status breakdown:\n",
      "  success: 2865 (96.0%)\n",
      "  not_found_in_json: 120 (4.0%)\n",
      "\n",
      "Article text length statistics (for articles with text):\n",
      "  Mean: 5004 characters\n",
      "  Median: 3769 characters\n",
      "  Min: 276 characters\n",
      "  Max: 32758 characters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_url(url):\n",
    "    \"\"\"Normalize URL for consistent matching (should match your original function)\"\"\"\n",
    "    if not url:\n",
    "        return url\n",
    "    \n",
    "    # Remove trailing slash\n",
    "    url = url.rstrip('/')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    url = url.lower()\n",
    "    \n",
    "    # Remove common tracking parameters\n",
    "    tracking_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_content', 'utm_term', \n",
    "                      'fbclid', 'gclid', 'ref', 'source']\n",
    "    \n",
    "    if '?' in url:\n",
    "        base_url, params = url.split('?', 1)\n",
    "        param_pairs = params.split('&')\n",
    "        filtered_params = []\n",
    "        \n",
    "        for param in param_pairs:\n",
    "            if '=' in param:\n",
    "                key = param.split('=')[0]\n",
    "                if key not in tracking_params:\n",
    "                    filtered_params.append(param)\n",
    "        \n",
    "        if filtered_params:\n",
    "            url = base_url + '?' + '&'.join(filtered_params)\n",
    "        else:\n",
    "            url = base_url\n",
    "    \n",
    "    return url\n",
    "\n",
    "def get_url_hash(url):\n",
    "    \"\"\"Generate hash for URL (should match your original function)\"\"\"\n",
    "    normalized = normalize_url(url)\n",
    "    return hashlib.md5(normalized.encode('utf-8')).hexdigest()\n",
    "\n",
    "def load_article_data(json_dir):\n",
    "    \"\"\"\n",
    "    Load all article JSON files and create a mapping from normalized URL to article data.\n",
    "    \n",
    "    Args:\n",
    "        json_dir: Directory containing the JSON files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping normalized URLs to article data\n",
    "    \"\"\"\n",
    "    articles = {}\n",
    "    json_files = list(Path(json_dir).glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Loading {len(json_files)} article files...\")\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Loading articles\"):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                article_data = json.load(f)\n",
    "            \n",
    "            # Use normalized_url if available, otherwise normalize the url\n",
    "            if 'normalized_url' in article_data:\n",
    "                normalized_url = article_data['normalized_url']\n",
    "            else:\n",
    "                normalized_url = normalize_url(article_data.get('url', ''))\n",
    "            \n",
    "            articles[normalized_url] = article_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def merge_csv_with_articles(csv_path, json_dir, output_path=None):\n",
    "    \"\"\"\n",
    "    Merge CSV data with article text from JSON files.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the original CSV file\n",
    "        json_dir: Directory containing article JSON files\n",
    "        output_path: Path for output CSV (optional, will auto-generate if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with merged data\n",
    "    \"\"\"\n",
    "    # Load the original CSV\n",
    "    print(\"Loading original CSV...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df)} rows from CSV\")\n",
    "    \n",
    "    # Load article data\n",
    "    articles = load_article_data(json_dir)\n",
    "    print(f\"Loaded {len(articles)} article files\")\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['article_title'] = ''\n",
    "    df['article_text'] = ''\n",
    "    df['article_source'] = ''\n",
    "    df['article_status'] = ''\n",
    "    df['article_retrieved_at'] = ''\n",
    "    df['has_article_text'] = False\n",
    "    \n",
    "    # Track matching statistics\n",
    "    matched_count = 0\n",
    "    unmatched_urls = []\n",
    "    \n",
    "    # Merge data\n",
    "    print(\"Merging article data with CSV...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Merging data\"):\n",
    "        # Try to find article data using normalized_url first, then url\n",
    "        article_data = None\n",
    "        \n",
    "        if pd.notna(row.get('normalized_url')):\n",
    "            article_data = articles.get(row['normalized_url'])\n",
    "        \n",
    "        if not article_data and pd.notna(row.get('url')):\n",
    "            # Try with the original URL normalized\n",
    "            normalized_original = normalize_url(row['url'])\n",
    "            article_data = articles.get(normalized_original)\n",
    "        \n",
    "        if article_data:\n",
    "            df.at[idx, 'article_title'] = article_data.get('title', '')\n",
    "            df.at[idx, 'article_text'] = article_data.get('text', '')\n",
    "            df.at[idx, 'article_source'] = article_data.get('source', '')\n",
    "            df.at[idx, 'article_status'] = article_data.get('status', '')\n",
    "            df.at[idx, 'article_retrieved_at'] = article_data.get('retrieved_at', '')\n",
    "            df.at[idx, 'has_article_text'] = bool(article_data.get('text', '').strip())\n",
    "            matched_count += 1\n",
    "        else:\n",
    "            df.at[idx, 'article_status'] = 'not_found_in_json'\n",
    "            unmatched_urls.append(row.get('url', 'No URL'))\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nMerging Statistics:\")\n",
    "    print(f\"  Total rows: {len(df)}\")\n",
    "    print(f\"  Matched with article data: {matched_count}\")\n",
    "    print(f\"  Unmatched: {len(df) - matched_count}\")\n",
    "    print(f\"  Rows with article text: {df['has_article_text'].sum()}\")\n",
    "    \n",
    "    if unmatched_urls:\n",
    "        print(f\"\\nFirst 5 unmatched URLs:\")\n",
    "        for url in unmatched_urls[:5]:\n",
    "            print(f\"  {url}\")\n",
    "        if len(unmatched_urls) > 5:\n",
    "            print(f\"  ... and {len(unmatched_urls) - 5} more\")\n",
    "    \n",
    "    # Save to CSV if output path provided\n",
    "    if output_path:\n",
    "        print(f\"\\nSaving merged data to {output_path}...\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(\"Saved successfully!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_merge_results(df):\n",
    "    \"\"\"\n",
    "    Analyze the results of the merge operation.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MERGE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    has_text = df['has_article_text'].sum()\n",
    "    no_text = total_rows - has_text\n",
    "    \n",
    "    print(f\"Total articles: {total_rows}\")\n",
    "    print(f\"With text: {has_text} ({has_text/total_rows*100:.1f}%)\")\n",
    "    print(f\"Without text: {no_text} ({no_text/total_rows*100:.1f}%)\")\n",
    "    \n",
    "    # Status breakdown\n",
    "    print(f\"\\nStatus breakdown:\")\n",
    "    status_counts = df['article_status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/total_rows*100:.1f}%)\")\n",
    "    \n",
    "    # Text length statistics for articles with text\n",
    "    text_lengths = df[df['has_article_text']]['article_text'].str.len()\n",
    "    if len(text_lengths) > 0:\n",
    "        print(f\"\\nArticle text length statistics (for articles with text):\")\n",
    "        print(f\"  Mean: {text_lengths.mean():.0f} characters\")\n",
    "        print(f\"  Median: {text_lengths.median():.0f} characters\")\n",
    "        print(f\"  Min: {text_lengths.min()} characters\")\n",
    "        print(f\"  Max: {text_lengths.max()} characters\")\n",
    "\n",
    "\n",
    "# Update these paths according to your setup\n",
    "CSV_PATH = \"data/cn-gmmp-mediacloud-urls.csv\"  # Path to your original CSV\n",
    "JSON_DIR = \"raw_articles_data\"  # Directory containing your JSON files\n",
    "OUTPUT_PATH = \"merged_articles.csv\"  # Output file path\n",
    "\n",
    "# Perform the merge\n",
    "merged_df = merge_csv_with_articles(CSV_PATH, JSON_DIR, OUTPUT_PATH)\n",
    "\n",
    "# Analyze results\n",
    "analyze_merge_results(merged_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
